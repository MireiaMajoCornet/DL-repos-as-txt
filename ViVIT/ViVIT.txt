

=== File: /home/ndelafuente/scenic/scenic/projects/vivit/model_utils.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Model utils for ViViT."""

from typing import Any, Optional, Tuple

from absl import logging
import flax
from flax.linen import linear
from flax.training import common_utils
import jax
import jax.numpy as jnp
import ml_collections
import numpy as np
from scenic.common_lib import debug_utils
from scenic.model_lib.base_models import model_utils as base_model_utils
import scipy


def reshape_to_1d_factorized(x: jnp.ndarray, axis: int):
  """Converts 2d inputs to 1d for axial attention."""

  assert x.ndim == 4, ('The input dimention should be '
                       '[batch_size, height, width, channel]')
  batch_size, height, width, channel = x.shape
  if axis == 1:
    return x.transpose((0, 2, 1, 3)).reshape(batch_size * width, height,
                                             channel)
  elif axis == 2:
    return x.reshape(batch_size * height, width, channel)


def reshape_to_2d_factorized(x: jnp.ndarray, axis: int,
                             two_d_shape: Tuple[int, int, int, int]):
  """Converts 1d inputs back to 2d after axial attention."""
  assert x.ndim == 3, ('The input dimention should be '
                       '[batch_size, height*width, channel]')
  batch_size, height, width, channel = two_d_shape
  if axis == 1:
    assert x.shape[0] == batch_size * width
    return x.reshape((batch_size, width, height, channel)).transpose(
        (0, 2, 1, 3))
  elif axis == 2:
    assert x.shape[0] == batch_size * height
    return x.reshape(two_d_shape)


def factorized_dot_product_attention(
    query: jnp.ndarray,
    key: jnp.ndarray,
    value: jnp.ndarray,
    bias: Optional[jnp.ndarray] = None,
    broadcast_dropout: bool = True,
    dropout_rng: Optional[Any] = None,
    dropout_rate: float = 0.1,
    deterministic: bool = False,
    dtype: jnp.dtype = jnp.float32,
    precision: Optional[jax.lax.Precision] = None,
) -> jnp.ndarray:
  """Applies head-factorized qkv dot-product attention.

  This factorizes the dot-product attention by assigning different
  heads to run attention on different axes.


  Args:
    query: Queries for calculating attention with shape of `[batch...,
      num_heads, qk_depth_per_head]`.
    key: Keys for calculating attention with shape of `[batch..., num_heads,
      qk_depth_per_head]`.
    value: Values to be used in attention with shape of `[batch..., num_heads,
      v_depth_per_head]`.
    bias: Bias for the attention weights. This should be
      broadcastable to the shape: `[batch...]`. This can be used for
        incorporating causal masks, padding masks, proximity bias, etc. Default
        is None, which means no bias is applied on attention matrix.
    broadcast_dropout: Use a broadcasted dropout along batch dims.
    dropout_rng: JAX PRNGKey to be used for dropout.
    dropout_rate: Dropout rate.
    deterministic: Deterministic or not (to apply dropout).
    dtype: The dtype of the computation (default: float32).
    precision: Numerical precision of the computation see `jax.lax.Precision`
      for details.

  Returns:
    Output of shape `[bs, ..., num_heads, features]`.
  """
  if query.shape != key.shape:
    raise ValueError('Axial dot product attention only supports '
                     'query and key with the same shape.')

  if bias is not None:
    raise ValueError('Bias is not supported in '
                     'factorized_dot_product_attention.')

  # Normalize the query with the square of its depth.
  query = query / jnp.sqrt(query.shape[-1]).astype(dtype)
  # Shape of query, key, and value: [bs, t, hw, h, c].

  prefix_str = 'abcdefghijk'
  # Split heads for each axial attention dimension.
  num_attn_dimensions = query.ndim - 3  # all dims but bs, heads, and channel.
  if query.shape[-2] % num_attn_dimensions != 0:
    raise ValueError(f'In head-axial dot-product attention, number of '
                     f'heads ({query.shape[-2]}) should be divisible by number '
                     f'of attention dimensions ({num_attn_dimensions})!')

  queries = jnp.split(query, num_attn_dimensions, axis=-2)
  keys = jnp.split(key, num_attn_dimensions, axis=-2)
  values = jnp.split(value, num_attn_dimensions, axis=-2)
  # queries, keys, and values are each a list with two arrays (sinec
  # we have two dims, t and hw) that are made by spliting heads:
  # [(bs, t, hw, h//2, c), (bs, t, hw, h//2, c)].

  outputs = []
  for i, (query, key, value) in enumerate(zip(queries, keys, values)):
    # Shape of query, key, and value: [bs, t, hw, h//2, c].
    axis = i + 1  # to account for the batch dim
    batch_dims = prefix_str[:axis]
    einsum_str = f'{batch_dims}x...z,{batch_dims}y...z->{batch_dims}x...y'
    # For axis=1 einsum_str (q,k->a): ax...z,ay...z->ax...y
    # For axis=2 einsum_str (q,k->a): abx...z,aby...z->abx...y
    attn_logits = jnp.einsum(einsum_str, query, key, precision=precision)
    # For axis=1 (attention over t): attn_logits.shape: [bs, t, hw, h//2, t]
    # For axis=2 (attention over hw): attn_logits.shape: [bs, t, hw, h//2, hw]
    attn_weights = jax.nn.softmax(attn_logits, axis=-1)

    # Apply dropout.
    if not deterministic and dropout_rate > 0.:
      if dropout_rng is None:
        raise ValueError('Did not provide `rng` to dot_product_attention().')
      keep_prob = 1.0 - dropout_rate
      if broadcast_dropout:
        # Dropout is broadcast across the batch+head+non-attention dimension.
        dropout_shape = list(attn_weights.shape)
        dropout_shape[0] = 1  # Broadcast batch.
        dropout_shape[-2] = 1  # Broadcast heads.
        keep = jax.random.bernoulli(dropout_rng, keep_prob, dropout_shape)
      else:
        keep = jax.random.bernoulli(dropout_rng, keep_prob, attn_weights.shape)
      multiplier = (
          keep.astype(attn_weights.dtype) /
          jnp.asarray(keep_prob, dtype=attn_weights.dtype))
      attn_weights *= multiplier

    einsum_str = f'{batch_dims}x...y,{batch_dims}y...z->{batch_dims}x...z'
    # For axis=1 einsum_str (a,v->o): ax...y,ay...z->ax...z
    # For axis=2 einsum_str (a,v->o): abx...y,aby...z->abx...z
    outputs.append(
        jnp.einsum(einsum_str, attn_weights, value, precision=precision))

  # Output is list with two arrays [(bs, t, hw, h//2, c), (bs, t, hw, h//2, c)]
  # concatinate the heads.
  return jnp.concatenate(outputs, axis=-2)


def central_frame_initializer():
  """Initialisation function for 3D convolutional kernels.

  The filter is initialised such that it only depends on the input at the
  central (w.r.t the time dimension) frame.

  Returns:
    init: Initialisation function for Flax
  """

  def init(key, shape, dtype=jnp.float32):
    assert len(shape) == 5, ('Should be initialising 5-d kernels'
                             '(t, h, w, c_in, c_out')
    init_kernel = linear.default_kernel_init(key, shape, dtype)
    central_time_index = shape[0] // 2
    init_kernel = init_kernel.at[:, :, :central_time_index, :, :].set(0.0)
    init_kernel = init_kernel.at[:, :, central_time_index + 1:, :, :].set(0.0)

    return init_kernel

  return init


def average_frame_initializer():
  """Initialisation function for 3D convolutional kernels.

  The filter is initialised such that it applies the same weights on each
  frame of the input.
  This is similar to "filter inflation" in
    "Joao Carreira, and Andrew Zisserman.
     Quo vadis, action recognition? a new model and the kinetics dataset".
  However, "filter inflation" uses the filter weights from a pretrained 2D CNN,
  and replicates them over all time dimensions.

  Returns:
    init: Initialisation function for Flax
  """

  def init(key, shape, dtype=jnp.float32):
    logging.info('Initialising shape %s', shape)
    assert len(shape) == 5, ('Should be initialising 5-d kernels'
                             '(t, h, w, c_in, c_out')
    assert shape[0] > 1, 'Temporal dimension should be > 1'

    # Tiling the temporal dimension of a larger kernel ensures that the
    # normalisation is handled by default_kernel_init().
    init_kernel = linear.default_kernel_init(key, shape, dtype)
    init_kernel = jnp.tile(init_kernel[0:1, :, :, :, :],
                           [init_kernel.shape[0], 1, 1, 1, 1])

    return init_kernel

  return init


def interpolate_positional_embeddings(restored_posemb_grid, n_tokens):
  """Interpolate positional embeddings from one size to another.

  Args:
    restored_posemb_grid: Positional embeddings from restored model. Shape is
      [n_restored_tokens, d]. It is assumed that the restored model used square
      image patches.
    n_tokens: Number of tokens in the target model. It is assumed that the input
      patches and image of the target model are square.

  Returns:
    positional embedding resized to match n_tokens. Shape is [1, n_tokens, d]
  """

  restored_gs = int(np.sqrt(len(restored_posemb_grid)))
  gs = int(np.sqrt(n_tokens))
  logging.info('Resizing grid-size from %s to %s.', restored_gs, gs)
  restored_posemb_grid = restored_posemb_grid.reshape(restored_gs, restored_gs,
                                                      -1)
  zoom = (gs / restored_gs, gs / restored_gs, 1)
  restored_posemb_grid = scipy.ndimage.zoom(restored_posemb_grid, zoom, order=1)
  restored_posemb_grid = restored_posemb_grid.reshape(1, gs * gs, -1)
  return restored_posemb_grid


def tile_positional_embeddings(restored_posemb_grid, n_tokens):
  """Tile positional embeddings.

  Args:
    restored_posemb_grid: Positional embeddings from restored model. Shape is
      [n_restored_tokens, d]
    n_tokens: Number of tokens in the target model.

  Returns:
    positional embedding tiled to match n_tokens. Shape is [1, n_tokens, d]
  """

  num_repeats = int(n_tokens / len(restored_posemb_grid))
  logging.info('Tiling loaded positional embeddings (%d), %d times',
               len(restored_posemb_grid), num_repeats)
  restored_posemb_grid = np.concatenate(
      [restored_posemb_grid] * num_repeats, axis=0)
  restored_posemb_grid = np.expand_dims(restored_posemb_grid, axis=0)

  return restored_posemb_grid


def interpolate_1d_positional_embeddings(restored_posemb, n_tokens):
  """Interpolate one-dimensional positional embeddings.

  Used when the number of tokens at the input of the encoder is different
  between the pretrained and target models. This function is used for the
  temporal encoder in the Factorised Encoder model which has 1d positional
  embeddings.

  Args:
    restored_posemb: Positional embeddings from restored model. Shape is
      [n_restored_tokens, d].
    n_tokens: Number of tokens in the target model.

  Returns:
    positional embedding tiled to match n_tokens. Shape is [1, n_tokens, d].
  """

  zoom = (n_tokens / restored_posemb.shape[0], 1)
  logging.info('Interpolating embeddings by a factor of %s', zoom)
  restored_posemb = scipy.ndimage.zoom(restored_posemb, zoom, order=1)
  restored_posemb = np.expand_dims(restored_posemb, axis=0)

  return restored_posemb


def initialise_from_train_state(
    config,
    train_state: Any,
    restored_train_state: Any,
    restored_model_cfg: ml_collections.ConfigDict,
    restore_output_proj: bool,
    vivit_transformer_key: str = 'Transformer',
    log_initialised_param_shapes: bool = True) -> Any:
  """Updates the train_state with data from restored_train_state.

  This function is written to be used for 'fine-tuning' experiments. Here, we
  do some surgery to support larger resolutions (longer sequence length) in
  the transformer block, with respect to the learned pos-embeddings.

  Args:
    config: Configurations for the model being updated.
    train_state: A raw TrainState for the model.
    restored_train_state: A TrainState that is loaded with parameters/state of a
      pretrained model.
    restored_model_cfg: Configuration of the model from which the
      restored_train_state come from. Usually used for some asserts.
    restore_output_proj: If true, load the final output projection. Set
      to False if finetuning to a new dataset.
    vivit_transformer_key: The key used for storing the subtree in the
      parameters that keeps Transformer weights, that are supposed to be
      initialized from the given pre-trained model.
    log_initialised_param_shapes: If true, print tabular summary of all the
      variables in the model once they have been initialised.

  Returns:
    Updated train_state.
  """
  if hasattr(train_state, 'optimizer'):
    # Inspect and compare the parameters of the model with the init-model.
    params = flax.core.unfreeze(train_state.optimizer.target)
    train_state_keys = train_state.optimizer.target.keys()
  else:
    params = flax.core.unfreeze(train_state.params)
    train_state_keys = train_state.params.keys()
  if hasattr(restored_train_state, 'optimizer'):
    if config.init_from.get('checkpoint_format', 'scenic') == 'big_vision':
      restored_params = restored_train_state.optimizer['target']
    else:
      restored_params = restored_train_state.optimizer.target
    restored_params = flax.core.unfreeze(restored_params)
  else:
    restored_params = flax.core.unfreeze(restored_train_state.params)

  # Start moving parameters, one-by-one and apply changes if needed.
  for m_key, m_params in restored_params.items():
    if m_key == 'output_projection':
      if restore_output_proj:
        params[m_key] = m_params
      else:
        pass

    elif m_key == 'pre_logits':
      if config.model.representation_size is None:
        # We don't have representation_size in the new model, so let's ignore
        #   if from the pretained model, in case it has it.
        # Note, removing the key from the dictionary is necessary to prevent
        #   obscure errors from the Flax optimizer.
        params.pop(m_key, None)
      else:
        assert restored_model_cfg.model.representation_size
        params[m_key] = m_params

    elif m_key in {'Transformer', 'SpatialTransformer', 'TemporalTransformer'}:
      key_to_load = vivit_transformer_key
      is_temporal = False
      if m_key == 'TemporalTransformer':
        key_to_load = m_key
        is_temporal = True
      for tm_key, tm_params in m_params.items():
        if tm_key == 'posembed_input':  # Might need resolution change.
          init_posemb(params[key_to_load], m_params, config, restored_model_cfg,
                      is_temporal=is_temporal)
        elif 'encoderblock' in tm_key:
          init_encoderblock(params[key_to_load], m_params, tm_key,
                            config)
        else:  # Other parameters of the Transformer encoder.
          params[key_to_load][tm_key] = tm_params

    elif m_key == 'embedding':
      init_embedding(params, m_params, config)
    else:
      if m_key in train_state_keys:
        params[m_key] = m_params
      else:
        logging.info('Skipping %s. In restored model but not in target', m_key)

  if log_initialised_param_shapes:
    logging.info('Parameter summary after initialising from train state')
    debug_utils.log_param_shapes(params)
  if hasattr(train_state, 'optimizer'):
    return train_state.replace(
        optimizer=train_state.optimizer.replace(
            target=flax.core.freeze(params)))
  else:
    return train_state.replace(params=flax.core.freeze(params))


def init_posemb(to_params,
                from_params,
                config,
                restored_model_cfg,
                is_temporal,
                posemb_name='posembed_input',
                restored_posemb_name='posembed_input'):
  """Initialize the positional embeddings."""
  if config.init_from.get('restore_positional_embedding', True):
    posemb = to_params[posemb_name]['pos_embedding']
    restored_posemb = from_params[restored_posemb_name]['pos_embedding']
    if restored_posemb.shape != posemb.shape:
      # Rescale the grid of pos, embeddings.
      # Default parameter shape is (1, N, 768)
      logging.info('Adapting positional embeddings from %s to %s',
                   restored_posemb.shape, posemb.shape)
      ntok = posemb.shape[1]
      if restored_model_cfg.model.classifier == 'token':
        # The first token is the CLS token.
        restored_posemb_grid = restored_posemb[0, 1:]
        if config.model.classifier == 'token':
          # CLS token in restored model and in target.
          cls_tok = restored_posemb[:, :1]
          ntok -= 1
        else:
          # CLS token in restored model, but not target.
          cls_tok = restored_posemb[:, :0]
      else:
        restored_posemb_grid = restored_posemb[0]
        if config.model.classifier == 'token':
          # CLS token in target, but not restored model.
          cls_tok = posemb[:, :1]
          ntok -= 1
        else:
          # CLS token not in target or restored model.
          cls_tok = restored_posemb[:, :0]
      if ((config.model.classifier == 'token') !=
          (restored_model_cfg.model.classifier == 'token')):
        logging.warning('Only one of target and restored model uses a '
                        'classification token.')

      if len(restored_posemb_grid) != ntok:  # We need a resolution change.
        if is_temporal:
          restored_posemb_grid = interpolate_1d_positional_embeddings(
              restored_posemb_grid, ntok)

        elif config.init_from.positional_embed_size_change == 'resize':
          restored_posemb_grid = interpolate_positional_embeddings(
              restored_posemb_grid, ntok)

        elif config.init_from.positional_embed_size_change == 'tile':
          restored_posemb_grid = tile_positional_embeddings(
              restored_posemb_grid, ntok)

        elif config.init_from.positional_embed_size_change == 'resize_tile':
          temp_encoding = config.model.temporal_encoding_config
          if temp_encoding.method == 'temporal_sampling':
            tokens_per_frame = int(ntok / temp_encoding.n_sampled_frames)
          elif temp_encoding.method == '3d_conv':
            n_frames = (
                config.dataset_configs.num_frames //
                config.model.patches.size[2])
            tokens_per_frame = ntok // n_frames
          else:
            raise AssertionError(
                f'Unknown temporal encoding {temp_encoding.method}')
          restored_posemb_grid = interpolate_positional_embeddings(
              restored_posemb_grid, tokens_per_frame)
          restored_posemb_grid = restored_posemb_grid[0]
          restored_posemb_grid = tile_positional_embeddings(
              restored_posemb_grid, ntok)

        else:
          raise AssertionError(
              'Unknown positional embedding size changing method')
      else:  # Sequence lengths are the same.
        # Adds batch dimension.
        restored_posemb_grid = restored_posemb_grid[None, ...]

      # Attach the CLS token again.
      if config.model.classifier == 'token':
        restored_posemb = jnp.array(
            np.concatenate([cls_tok, restored_posemb_grid], axis=1))
      else:
        restored_posemb = restored_posemb_grid

    to_params[posemb_name]['pos_embedding'] = restored_posemb
  else:
    logging.info('Not restoring positional encodings from pretrained model')


def init_encoderblock(to_params, from_params, tm_key, config):
  """Initialize encoder_block_parameters."""
  # Explicitly enumerate over the keys in the encoder-block. Don't just
  # assign the dictionary. It is possible for the target model to
  # contain keys that are not in the restored model.
  attention_type = config.model.attention_config.type
  for enc_key in from_params[tm_key].keys():
    if attention_type in [
        'spacetime', 'factorized_encoder', 'factorized_dot_product_attention'
    ]:
      assert enc_key in to_params[tm_key], '%s not in to_params[%s]' % (enc_key,
                                                                        tm_key)
      to_params[tm_key][enc_key] = from_params[tm_key][enc_key]

    elif attention_type == 'factorized_transformer_block':
      if config.init_from.get('init_spatial_transformer', True):
        to_params[tm_key]['encoderblock_space'] = from_params
      if config.init_from.get('init_temporal_transformer', True):
        to_params[tm_key]['encoderblock_time'] = from_params

    elif attention_type == 'factorized_self_attention_block':
      if enc_key in to_params[tm_key]:
        # We have an exact match. This would happen when loading weights from
        # another factorised encoder model.
        to_params[tm_key][enc_key] = from_params[tm_key][enc_key]
        logging.info('%s: Initialising %s directly from restored model', tm_key,
                     enc_key)
      elif enc_key == 'MultiHeadDotProductAttention_0':
        if config.init_from.get('init_spatial_transformer', True):
          logging.info(
              '%s: Initialising spatial transformer from '
              'pretrained weights', tm_key)
          to_params[tm_key]['MultiHeadDotProductAttention_space'] = from_params[
              tm_key][enc_key].copy()
        if config.init_from.get('init_temporal_transformer', True):
          logging.info(
              '%s: Initialising temporal transformer from '
              'pretrained weights', tm_key)
          to_params[tm_key]['MultiHeadDotProductAttention_time'] = from_params[
              tm_key][enc_key].copy()
      elif enc_key == 'LayerNorm_0':
        to_params[tm_key]['LayerNorm_space'] = from_params[tm_key][enc_key]
        if config.init_from.get('init_temporal_layer_norm', False):
          logging.info(
              '%s: %s Initialising temporal layer norm from '
              'restored model', tm_key, enc_key)
      # The following part could be made more generic.
      elif enc_key == 'LayerNorm_1':
        to_params[tm_key]['LayerNorm_mlp'] = from_params[tm_key][enc_key]
      elif enc_key == 'MlpBlock_0':
        to_params[tm_key]['MlpBlock'] = from_params[tm_key][enc_key]
      else:
        logging.info(
            'Key "%s" in restored model\'s encoder block not in '
            'target model', enc_key)
    else:
      raise ValueError(f'Unknown attention type {attention_type}')


def init_embedding(to_params, from_params, config):
  """Initialize input embedding."""
  if config.init_from.get('restore_input_embedding', True):
    input_kernel = to_params['embedding']['kernel']
    restored_kernel = from_params['kernel']
    restored_bias = from_params['bias']

    if input_kernel.shape != restored_kernel.shape:
      # Kernel dimensions are [t, h, w, c_in, c_out].
      assert config.model.temporal_encoding_config.method == '3d_conv', (
          'Input kernel dimensions should only differ if 3d_conv is the'
          'temporal encoding method')
      assert input_kernel.shape[1:] == restored_kernel.shape, (
          'All filter dimensions besides the temporal dimension should be'
          'equal. {} vs {}'.format(input_kernel.shape, restored_kernel.shape))

      kernel_init_method = (
          config.model.temporal_encoding_config.kernel_init_method
      )
      if kernel_init_method == 'average_frame_initializer':
        # This corresponds to "filter inflation" in
        # J Carreira and A Zisserman. Quo vadis, action recognition?
        # A new model and the kinetics dataset. CVPR 2017".
        logging.info('Initializing input kernel with filter inflation.')
        t = input_kernel.shape[0]
        restored_kernel = np.expand_dims(restored_kernel, axis=0)
        restored_kernel = np.tile(restored_kernel, [t, 1, 1, 1, 1]) / t
      elif kernel_init_method == 'average_arp_frame_initializer':
        # This corresponds to a combination of filter inflation and
        # the approximate rank pooling described in
        # H Bilen et al. Action Recognition with Dynamic Image Networks.
        # PAMI 2017.
        logging.info('Initialzing input kernel with ARP inflation')
        t = input_kernel.shape[0]
        restored_kernel = np.expand_dims(restored_kernel, axis=0)
        restored_kernel = np.tile(restored_kernel, [t, 1, 1, 1, 1])

        def average_arp(length):
          # Implements Equation 3 of Bilen et al. PAMI 2017.
          array = np.arange(1, length + 1)

          harmonic = np.zeros((length + 1))
          harmonic[1:] = np.cumsum(1.0 / array)

          array = 2 * (length - array + 1) - (length + 1) * (
              harmonic[-1] - harmonic[:-1])
          return array

        normalizer = average_arp(t) / t
        normalizer = np.reshape(normalizer, [t, 1, 1, 1, 1])
        restored_kernel = restored_kernel * normalizer
      elif kernel_init_method == 'central_frame_initializer':
        logging.info('Initializing input kernel to select centre frame.')
        central_time_index = input_kernel.shape[0] // 2
        temp = np.zeros(input_kernel.shape)
        temp[central_time_index] = restored_kernel.copy()
        restored_kernel = temp
      else:
        raise AssertionError(
            'Unknown input kernel initialization {}'.format(kernel_init_method))

    to_params['embedding']['kernel'] = restored_kernel
    to_params['embedding']['bias'] = restored_bias
  else:
    logging.info('Not restoring input embedding parameters')


def get_joint_logits_labels(logits, one_hot_targets, class_splits):
  """Returns joint pairs of logits and labels.

  Args:
    logits: Tensor of shape [n, c]
    one_hot_targets: Tensor of shape [n, c]
    class_splits: List of length 2. The two elements, c1 and c. Used in
      jnp.split. Size of the two splits is therefore c1 and (c - c1)

  Returns:
    pairwise_logits: Tensor of shape [n, c1 * c2]
    pairwise_labels: One-hot tensor of shape [n, c1 * c2]
  """

  assert len(class_splits) == 2, 'Class_splits should have length 2'
  assert logits.ndim == 2, 'Logits should have dimension of 2'
  assert one_hot_targets.ndim == 2, 'One hot target should have dimension of 2'

  n = logits.shape[0]

  logits_a, logits_b = jnp.split(logits, class_splits, axis=-1)[:-1]
  one_hot_a, one_hot_b = jnp.split(one_hot_targets, class_splits, axis=-1)[:-1]
  n_class_a, n_class_b = logits_a.shape[1], logits_b.shape[1]

  logits_a = jax.nn.softmax(logits_a, axis=-1)
  logits_b = jax.nn.softmax(logits_b, axis=-1)

  pairwise_logits = logits_a[:, :, jnp.newaxis] * logits_b[:, jnp.newaxis, :]
  pairwise_logits = jnp.reshape(pairwise_logits, [n, n_class_a * n_class_b])
  labels_a = jnp.argmax(one_hot_a, axis=-1)
  labels_b = jnp.argmax(one_hot_b, axis=-1)

  pairwise_labels = labels_a * n_class_b + labels_b
  pairwise_labels = common_utils.onehot(pairwise_labels, n_class_a * n_class_b)

  return pairwise_logits, pairwise_labels


def joint_accuracy(logits, one_hot_target, class_splits, weights=None):
  """Compute accuracy where both targets must be predicted correctly."""

  pairwise_logits, pairwise_labels = get_joint_logits_labels(
      logits, one_hot_target, class_splits)
  return base_model_utils.weighted_correctly_classified(pairwise_logits,
                                                        pairwise_labels,
                                                        weights)


def joint_top_k(logits, one_hot_target, class_splits, k=5, weights=None):
  """Compute top-k where both targets must be predicted correctly."""

  pairwise_logits, pairwise_labels = get_joint_logits_labels(
      logits, one_hot_target, class_splits)
  return base_model_utils.weighted_topk_correctly_classified(
      pairwise_logits, pairwise_labels, weights, k)


def adapt_old_configs(
    hparams: ml_collections.ConfigDict) -> ml_collections.ConfigDict:
  """Updates old configs with new namings."""
  with hparams.unlocked():
    # Make sure attention_config exists.
    attention_config = hparams.model.get('attention_config', None)
    if attention_config is None:
      hparams.model.attention_config = ml_collections.ConfigDict()
    att_type = hparams.model.attention_config.get('type', None)

    # Default ViViT.
    if att_type is None:
      hparams.model.attention_config.type = 'spacetime'

    # Handle V3.
    elif att_type == 'factorised_space_time':
      hparams.model.attention_config.type = 'factorized_self_attention_block'

    # Handle V1.
    if hparams.get('model_variant', 'vivit') == 'space_time_vivit':
      hparams.model.attention_config.type = 'factorized_encoder'

  return hparams



=== File: /home/ndelafuente/scenic/scenic/projects/vivit/trainer.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Training Script for ViViT."""

import copy
import functools
from typing import Any, Dict, Tuple

from absl import logging
from clu import metric_writers
from clu import periodic_actions
import flax
from flax import jax_utils
import jax
import jax.numpy as jnp
import jax.profiler
import ml_collections
import numpy as np
from scenic.dataset_lib import dataset_utils
from scenic.projects.vivit import evaluation_lib
from scenic.projects.vivit import train_utils as vivit_train_utils
from scenic.train_lib_deprecated import lr_schedules
from scenic.train_lib_deprecated import optimizers
from scenic.train_lib_deprecated import pretrain_utils
from scenic.train_lib_deprecated import train_utils


def train(
    *,
    rng: jnp.ndarray,
    config: ml_collections.ConfigDict,
    model_cls: Any,
    dataset: dataset_utils.Dataset,
    workdir: str,
    writer: metric_writers.MetricWriter,
) -> Tuple[train_utils.TrainState, Dict[str, Any], Dict[str, Any]]:
  """Main training loop lives in this function.

  Given the model class and dataset, it prepares the items needed to run the
  training, including the TrainState.

  Args:
    rng: Jax rng key.
    config: Configurations of the experiment.
    model_cls: Model class; A model has a flax_module, a loss_fn, and a
      metrics_fn associated with it.
    dataset: The dataset that has train_iter, eval_iter, meta_data, and
      optionally, test_iter.
    workdir: Directory for checkpointing.
    writer: CLU metrics writer instance.

  Returns:
    train_state that has the state of training (including current
      global_step, model_state, rng, and the optimizer), train_summary
      and eval_summary which are dict of metrics. These outputs are used for
      regression testing.
  """
  flax.config.update('flax_return_frozendict', True)
  lead_host = jax.process_index() == 0
  # Build the loss_fn, metrics, and flax_model.
  model = model_cls(config, dataset.meta_data)
  is_multilabel_model = (config.model_name == 'vivit_multilabel_classification')
  get_confusion_matrix = (config.get('confusion_matrix_metrics', False)
                          and not is_multilabel_model)

  # Initialize model.
  rng, init_rng = jax.random.split(rng)
  (params, model_state, num_trainable_params,
   gflops) = train_utils.initialize_model(
       model_def=model.flax_model,
       input_spec=[(dataset.meta_data['input_shape'],
                    dataset.meta_data.get('input_dtype', jnp.float32))],
       config=config,
       rngs=init_rng)

  # Create optimizer.
  # We jit this, such that the arrays that are created are created on the same
  # device as the input is, in this case the CPU. Else they'd be on device[0].
  optimizer = jax.jit(
      optimizers.get_optimizer(config).create, backend='cpu')(
          params)
  rng, train_rng = jax.random.split(rng)
  train_state = train_utils.TrainState(
      global_step=0,
      optimizer=optimizer,
      model_state=model_state,
      rng=train_rng,
      accum_train_time=0)
  start_step = train_state.global_step
  if config.checkpoint:
    train_state, start_step = train_utils.restore_checkpoint(
        workdir, train_state)

  if (start_step == 0  # Which means "no" checkpoint is restored!
      and config.get('init_from') is not None):
    restored_model_cfg = config.init_from.get('model_config')
    init_checkpoint_path = config.init_from.get('checkpoint_path')
    checkpoint_format = config.init_from.get('checkpoint_format', 'scenic')
    if checkpoint_format == 'scenic':
      restored_train_state = pretrain_utils.restore_pretrained_checkpoint(
          init_checkpoint_path, train_state, assert_exist=True)
    elif checkpoint_format == 'big_vision':
      restored_train_state = (
          pretrain_utils.convert_big_vision_to_scenic_checkpoint(
              init_checkpoint_path, train_state))
      # Config dict in big_vision is not the same format as scenic.
      # Therefore, make sure config match the config of the loaded model!
      restored_model_cfg = copy.deepcopy(config)
      # The following is needed when the restored and target models used a
      # different classifier. As big_vision uses a different config dict, we
      # have to specify this manually.
      restored_model_cfg.model.classifier = config.init_from.get(
          'classifier_type', 'token')

    train_state = model.init_from_train_state(train_state, restored_train_state,
                                              restored_model_cfg)
    # Free unnecessary memory.
    del restored_train_state
  elif start_step == 0:
    logging.info('Training completely from scratch.'
                 'Not restoring from any checkpoint.')

  # Replicate the optimzier, state, and rng.
  train_state = jax_utils.replicate(train_state)
  del params  # Do not keep a copy of the initial params.
  # Calculate the total number of training steps.
  total_steps, steps_per_epoch = train_utils.get_num_training_steps(
      config, dataset.meta_data)
  # Get learning rate scheduler.
  learning_rate_fn = lr_schedules.get_learning_rate_fn(config)

  train_step_pmapped = jax.pmap(
      functools.partial(
          vivit_train_utils.train_step,
          flax_model=model.flax_model,
          learning_rate_fn=learning_rate_fn,
          loss_fn=model.loss_function,
          metrics_fn=model.get_metrics_fn('train'),
          config=config,
          debug=config.debug_train),
      axis_name='batch',
      # We can donate both buffers of train_state and train_batch.
      donate_argnums=(0, 1),
  )
  eval_step_pmapped = jax.pmap(
      functools.partial(
          vivit_train_utils.eval_step,
          flax_model=model.flax_model,
          metrics_fn=model.get_metrics_fn('validation'),
          return_logits_and_labels=is_multilabel_model,
          return_confusion_matrix=get_confusion_matrix,
          debug=config.debug_eval),
      axis_name='batch',
      # We can donate the eval_batch's buffer.
      donate_argnums=(1,),
  )
  log_eval_steps = config.get('log_eval_steps') or steps_per_epoch
  log_test_steps = 0
  if config.dataset_configs.get('do_multicrop_test'):
    log_test_steps = int(steps_per_epoch *
                         config.dataset_configs.log_test_epochs)

    test_step_pmapped = jax.pmap(
        functools.partial(
            vivit_train_utils.test_step,
            flax_model=model.flax_model,
            metrics_fn=model.get_metrics_fn('test'),
            n_clips=config.get('multicrop_clips_per_device', 2),
            debug=config.debug_eval),
        axis_name='batch',
        # We can donate the test_batch's buffer.
        donate_argnums=(1,),
    )

    assert config.dataset_configs.test_batch_size == jax.local_device_count(), (
        'The per-host batch size must be equal to the number of local devices.'
        'This ensures that each TPU device is processing different views of'
        'the same original video.')

    total_test_steps = int(
        np.ceil(dataset.meta_data['num_test_examples'] /
                (config.get('dataset_configs.test_batch_size') *
                 config.get('dataset_configs.num_test_clips') *
                 jax.process_count())))
    steps_per_test = config.get('steps_per_test') or total_test_steps

  if not log_eval_steps:
    raise ValueError("'log_eval_steps' should be specified in the config.")
  checkpoint_steps = config.get('checkpoint_steps') or log_eval_steps
  log_summary_steps = config.get('log_summary_steps') or log_eval_steps

  # Ceil rounding such that we include the last incomplete batch.
  eval_batch_size = config.get('eval_batch_size', config.batch_size)
  total_eval_steps = int(
      np.ceil(dataset.meta_data['num_eval_examples'] / eval_batch_size))
  steps_per_eval = config.get('steps_per_eval') or total_eval_steps

  train_metrics, extra_training_logs = [], []
  train_summary, eval_summary = None, None

  chrono = train_utils.Chrono(
      first_step=start_step,
      total_steps=total_steps,
      steps_per_epoch=steps_per_epoch,
      global_bs=config.batch_size,
      accum_train_time=int(jax_utils.unreplicate(train_state.accum_train_time)))

  logging.info('Starting training loop at step %d.', start_step + 1)
  report_progress = periodic_actions.ReportProgress(
      num_train_steps=total_steps, writer=writer)
  hooks = []
  if lead_host:
    hooks.append(report_progress)
  if config.get('xprof', True) and lead_host:
    hooks.append(periodic_actions.Profile(num_profile_steps=5, logdir=workdir))

  if start_step == 0:
    step0_log = {'num_trainable_params': num_trainable_params}
    if gflops:
      step0_log['gflops'] = gflops
    writer.write_scalars(1, step0_log)

  # Manually defragment memory before starting training, if we are using the
  # tfrt runtime.
  do_memory_defrag = False
  if config.get('do_memory_defrag', False):
    client = jax.lib.xla_bridge.get_backend()
    try:
      logging.info('Defragmenting memory')
      client.defragment()
      do_memory_defrag = True
    except RuntimeError:
      logging.warn('Memory defragmentation not possible, use the tfrt runtime')
  for step in range(start_step + 1, total_steps + 1):
    with jax.profiler.StepTraceAnnotation('train', step_num=step):
      train_batch = next(dataset.train_iter)
      train_state, t_metrics, lr = train_step_pmapped(train_state, train_batch)
      # This will accumulate metrics in TPU memory up to the point that we log
      # them. This is no problem for small metrics but may be a problem for
      # large (e.g. segmentation) metrics. An alternative is to set
      # `log_summary_steps` to a small number, or to use
      # `train_utils.unreplicate_and_get` here instead of right before writing
      # summaries, but that means in each step, we have data transfer between
      # tpu and host, which might slow down the training.
      train_metrics.append(t_metrics)
      # Additional training logs: learning rate:
      extra_training_logs.append({'learning_rate': lr})

    for h in hooks:
      # Catch exception in case XProf fails.
      try:
        h(step)
      except ValueError as error:
        logging.exception('Hook failed: %r', error)

    chrono.pause()  # Below are once-in-a-while ops -> pause.
    ###################### LOG TRAIN SUMMARY ########################
    if (step % log_summary_steps == 1) or (step == total_steps):
      if lead_host:
        chrono.tick(step, writer=writer)
      train_summary = train_utils.log_train_summary(
          step=step,
          train_metrics=jax.tree_util.tree_map(train_utils.unreplicate_and_get,
                                               train_metrics),
          extra_training_logs=jax.tree_util.tree_map(
              train_utils.unreplicate_and_get, extra_training_logs),
          writer=writer,
          key_separator='/')
      # Reset metric accumulation for next evaluation cycle.
      train_metrics, extra_training_logs = [], []
      if do_memory_defrag:
        logging.info('Defragmenting memory')
        client.defragment()

    ################### EVALUATION ################################
    if (step % log_eval_steps == 1) or (step == total_steps):
      with report_progress.timed('eval'):
        if do_memory_defrag:
          logging.info('Defragmenting memory')
          client.defragment()

        eval_metrics = []
        additional_summary = None
        if is_multilabel_model:
          eval_logits = []
          eval_labels = []
          n_classes = dataset.meta_data['num_classes']
        if get_confusion_matrix:
          confusion_matrices = []
          n_classes = dataset.meta_data['num_classes']

        # Sync model state across replicas.
        train_state = train_utils.sync_model_state_across_replicas(train_state)
        for _ in range(steps_per_eval):
          eval_batch = next(dataset.valid_iter)
          e_metrics = eval_step_pmapped(train_state, eval_batch)
          if is_multilabel_model:
            e_metrics, logits_batch, labels_batch = e_metrics
            # TODO(dehghani, lucic): Fetching from the device in each step might
            #  be an unnecessary penalty. Consider updating to async fetching
            #  as in CL/378384754.
            eval_logits.append(vivit_train_utils.to_cpu(logits_batch))
            eval_labels.append(vivit_train_utils.to_cpu(labels_batch))
          if get_confusion_matrix:
            e_metrics, conf_matrix = e_metrics
            confusion_matrices.append(vivit_train_utils.to_cpu(conf_matrix))
          # Fetch e_metrics to host and store.
          eval_metrics.append(train_utils.unreplicate_and_get(e_metrics))

        # Compute global metrics if applicable from all the batches.
        if is_multilabel_model:
          additional_summary = evaluation_lib.compute_mean_average_precision(
              np.concatenate(eval_logits, axis=0),
              np.concatenate(eval_labels, axis=0),
              return_per_class_ap=n_classes < 10)
        if get_confusion_matrix:
          additional_summary = evaluation_lib.compute_confusion_matrix_metrics(
              confusion_matrices, return_per_class_metrics=n_classes < 10)
          if lead_host:
            conf_matrix_image = vivit_train_utils.render_confusion_matrices(
                confusion_matrices, normalization_method='rows')
            conf_matrix_unnorm = vivit_train_utils.render_confusion_matrices(
                confusion_matrices, normalization_method='none')

            writer.write_images(
                step, {'valid/conf_matrix': conf_matrix_image,
                       'valid/conf_matrix_unnormalized': conf_matrix_unnorm})

        # Log eval summary.
        eval_summary = train_utils.log_eval_summary(
            step=step,
            eval_metrics=eval_metrics,
            extra_eval_summary=additional_summary,
            writer=writer,
            key_separator='/')
        writer.flush()
        del eval_metrics
        if do_memory_defrag:
          logging.info('Defragmenting memory')
          client.defragment()

    ##################### CHECKPOINTING ###########################
    if ((step % checkpoint_steps == 0 and step > 0) or
        (step == total_steps)) and config.checkpoint:
      with report_progress.timed('checkpoint'):
        # Sync model state across replicas.
        train_state = train_utils.sync_model_state_across_replicas(train_state)
        if lead_host:
          train_state.replace(  # pytype: disable=attribute-error
              accum_train_time=chrono.accum_train_time)
          train_utils.save_checkpoint(workdir, train_state)

    ############# MULTICROP TESTING ############################
    if (config.dataset_configs.get('do_multicrop_test') and
        ((step % log_test_steps == 1 and step > 1) or step == total_steps)):
      with report_progress.timed('test'):
        if do_memory_defrag:
          logging.info('Defragmenting memory')
          client.defragment()

        test_metrics = []
        # Sync model state across replicas.
        train_state = train_utils.sync_model_state_across_replicas(train_state)

        # At the end of training, evaluate on the whole test set.
        if step == total_steps:
          steps_per_test = total_test_steps

        logging.info('Starting multicrop test')
        for _ in range(steps_per_test):
          test_batch = next(dataset.test_iter)
          t_metrics = test_step_pmapped(train_state, test_batch)
          # Fetch t_metrics to host and store.
          test_metrics.append(train_utils.unreplicate_and_get(t_metrics))
        # Log eval summary.
        train_utils.log_eval_summary(
            step=step,
            eval_metrics=test_metrics,
            writer=writer,
            prefix='test',
            key_separator='/')
        logging.info('Completed multicrop test')
        writer.flush()
        # Free up some space.
        del test_metrics
        if do_memory_defrag:
          logging.info('Defragmenting memory')
          client.defragment()

    chrono.resume()  # un-pause now
  # Wait until computations are done before exiting.
  train_utils.barrier_across_hosts()
  # Return the train and eval summary after last step for regression testing.
  return train_state, train_summary, eval_summary



=== File: /home/ndelafuente/scenic/scenic/projects/vivit/evaluation_lib.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Functions for evaluation."""

import os
from typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union

from absl import logging
import flax
from flax.metrics import tensorboard
from flax.training import checkpoints
import jax
import jax.numpy as jnp
import numpy as np
from scenic.train_lib_deprecated import pretrain_utils
from scenic.train_lib_deprecated import train_utils
from sklearn.metrics import average_precision_score
from tensorflow.io import gfile


# Aliases for custom types:
Array = Union[jnp.ndarray, np.ndarray]


def restore_checkpoint(checkpoint_path: str,
                       train_state: Optional[train_utils.TrainState] = None,
                       assert_exist: bool = False,
                       step: int = None) -> Tuple[train_utils.TrainState, int]:
  """Restores the last checkpoint.

  Supports checkpoints saved either with old Scenic (flax.deprecated.nn) or
  current Scenic (flax.Linen). Therefore, this function can be used for
  evaluation of old or current models.

  First restores the checkpoint, which is an instance of TrainState that holds
  the state of training, and then replicates it.

  Args:
    checkpoint_path: Directory for saving the checkpoint.
    train_state: An instance of TrainState that holds the state of training.
    assert_exist: bool; Assert that there is at least one checkpoint exists in
      the given path.
    step: Step number to load or None to load latest. If specified,
      checkpoint_path must be a directory.

  Returns:
    Training state and an int which is the current step.
  """
  if assert_exist:
    glob_path = os.path.join(checkpoint_path, 'checkpoint_*')
    if not gfile.glob(glob_path):
      raise ValueError('No checkpoint for the pretrained model is found in: '
                       f'{checkpoint_path}')
  restored_train_state = checkpoints.restore_checkpoint(checkpoint_path, None,
                                                        step)

  if restored_train_state:
    (restored_params, restored_model_state
    ) = pretrain_utils.get_params_and_model_state_dict(restored_train_state)
    restored_params = flax.core.freeze(restored_params)
    restored_model_state = flax.core.freeze(restored_model_state)
    train_state = train_state or train_utils.TrainState()
    if train_state.optimizer:
      new_optimizer = train_state.optimizer.replace(target=restored_params)
    else:
      new_optimizer = {'target': restored_params}
    train_state = train_state.replace(  # pytype: disable=attribute-error
        optimizer=new_optimizer,
        model_state=restored_model_state,
        global_step=int(restored_train_state['global_step']),
        rng=restored_train_state['rng'],
        accum_train_time=restored_train_state.get('accum_train_time', 0))
  else:
    train_state = train_state or train_utils.TrainState()

  return train_state, int(train_state.global_step)


def compute_mean_average_precision(logits, labels, suffix='',
                                   suffix_separator='_',
                                   return_per_class_ap=False):
  """Computes mean average precision for multi-label classification.

  Args:
    logits: Numpy array of shape [num_examples, num_classes]
    labels: Numpy array of shape [num_examples, num_classes]
    suffix: Suffix to add to the summary
    suffix_separator: Separator before adding the suffix
    return_per_class_ap: If True, return results for each class in the summary.

  Returns:
    summary: Dictionary containing the mean average precision, and also the
      average precision per class.
  """

  assert logits.shape == labels.shape, 'Logits and labels have different shapes'
  n_classes = logits.shape[1]
  average_precisions = []
  if suffix:
    suffix = suffix_separator + suffix
  summary = {}

  for i in range(n_classes):
    ave_precision = average_precision_score(labels[:, i], logits[:, i])
    if np.isnan(ave_precision):
      logging.warning('AP for class %d is NaN', i)

    if return_per_class_ap:
      summary_key = f'per_class_average_precision_{i}{suffix}'
      summary[summary_key] = ave_precision
    average_precisions.append(ave_precision)

  mean_ap = np.nanmean(average_precisions)
  summary[f'mean_average_precision{suffix}'] = mean_ap
  logging.info('Mean AP is %0.3f', mean_ap)

  return summary


def compute_confusion_matrix_metrics(
    confusion_matrices: Sequence[Array],
    return_per_class_metrics: bool) -> Dict[str, float]:
  """Computes classification metrics from a confusion matrix.

  Computes the recall, precision and jaccard index (IoU) from the input
  confusion matrices. The confusion matrices are assumed to be of the form
  [ground_truth, predictions]. In other words, ground truth classes along the
  rows, and predicted classes along the columns.

  Args:
    confusion_matrices: Sequence of [n_batch, n_class, n_class] confusion
      matrices. The first two dimensions will be summed over to get an
      [n_class, n_class] matrix for further metrics.
    return_per_class_metrics: If true, return per-class metrics.

  Returns:
    A dictionary of metrics (recall, precision and jaccard index).
  """

  conf_matrix = np.sum(confusion_matrices, axis=0)  # Sum over eval batches.
  if conf_matrix.ndim != 3:
    raise ValueError(
        'Expecting confusion matrix to have shape '
        f'[batch_size, num_classes, num_classes], got {conf_matrix.shape}.')
  conf_matrix = np.sum(conf_matrix, axis=0)  # Sum over batch dimension.
  n_classes = conf_matrix.shape[0]
  metrics_dict = {}

  # We assume that the confusion matrix is [ground_truth x predictions].
  true_positives = np.diag(conf_matrix)
  sum_rows = np.sum(conf_matrix, axis=0)
  sum_cols = np.sum(conf_matrix, axis=1)

  recall_per_class = true_positives / sum_cols
  precision_per_class = true_positives / sum_rows
  jaccard_index_per_class = (
      true_positives / (sum_rows + sum_cols - true_positives))

  metrics_dict['recall/mean'] = np.nanmean(recall_per_class)
  metrics_dict['precision/mean'] = np.nanmean(precision_per_class)
  metrics_dict['jaccard/mean'] = np.nanmean(jaccard_index_per_class)

  def add_per_class_results(metric: Array, name: str) -> None:
    for i in range(n_classes):
      # We set NaN values (from dividing by 0) to 0, to not cause problems with
      # logging.
      metrics_dict[f'{name}/{i}'] = np.nan_to_num(metric[i])

  if return_per_class_metrics:
    add_per_class_results(recall_per_class, 'recall')
    add_per_class_results(precision_per_class, 'precision')
    add_per_class_results(jaccard_index_per_class, 'jaccard')

  return metrics_dict


def prune_summary(summary, prefixes_to_remove):
  """Removes keys starting with provided prefixes from the dict."""
  ret = {}
  for key in summary.keys():
    report_key = True
    for prefix in prefixes_to_remove:
      if key.startswith(prefix):
        report_key = False
        break
    if report_key:
      ret[key] = summary[key]
  return ret




def log_eval_summary(step: int,
                     eval_metrics: Sequence[Dict[str, Tuple[float, int]]],
                     extra_eval_summary: Optional[Dict[str, Any]] = None,
                     summary_writer: Optional[Any] = None,
                     metrics_normalizer_fn: Optional[
                         Callable[[Dict[str, Tuple[float, int]], str],
                                  Dict[str, float]]] = None,
                     prefix: str = 'valid',
                     key_separator: str = '_') -> Dict[str, float]:
  """Computes and logs eval metrics.

  Args:
    step: Current step.
    eval_metrics: Sequence of dictionaries of calculated metrics.
    extra_eval_summary: A dict containing summaries that are already ready to be
      logged, e.g. global metrics from eval set, like precision/recall.
    summary_writer: Summary writer object.
    metrics_normalizer_fn: Used for normalizing metrics. The api for
      this function is: `new_metrics_dict = metrics_normalizer_fn( metrics_dict,
        split)`. If set to None, we use the normalize_metrics_summary which uses
        the normalizer paired with each metric to normalize it.
    prefix: str; Prefix added to the name of the summaries writen by this
      function.
    key_separator: Separator added between the prefix and key.

  Returns:
    eval summary: A dictionary of metrics.
  """
  eval_metrics = train_utils.stack_forest(eval_metrics)

  # Compute the sum over all examples in all batches.
  eval_metrics_summary = jax.tree_util.tree_map(lambda x: x.sum(), eval_metrics)
  # Normalize metrics by the total number of exampels.
  metrics_normalizer_fn = (
      metrics_normalizer_fn or train_utils.normalize_metrics_summary)
  eval_metrics_summary = metrics_normalizer_fn(eval_metrics_summary, 'eval')
  # If None, set to an empty dictionary.
  extra_eval_summary = extra_eval_summary or {}

  if jax.process_index() == 0:
    message = ''
    for key, val in eval_metrics_summary.items():
      message += f'{key}: {val} | '
    for key, val in extra_eval_summary.items():
      message += f'{key}: {val} | '
    logging.info('step: %d -- %s -- {%s}', step, prefix, message)

    if summary_writer is not None:
      for key, val in eval_metrics_summary.items():
        summary_writer.scalar(f'{prefix}{key_separator}{key}', val, step)
      for key, val in extra_eval_summary.items():
        summary_writer.scalar(f'{prefix}{key_separator}{key}', val, step)
      summary_writer.flush()

  # Add extra_eval_summary to the returned eval_summary.
  eval_metrics_summary.update(extra_eval_summary)
  return eval_metrics_summary



=== File: /home/ndelafuente/scenic/scenic/projects/vivit/requirements.txt ===


dmvr @ git+https://github.com/deepmind/dmvr.git
seaborn>=0.11.2



=== File: /home/ndelafuente/scenic/scenic/projects/vivit/README.md ===


ViViT: A Video Vision Transformer
==
![ViViT: A Video Vision Transformer](data/vivit.png)

ViViT is a family of pure-transformer based models for video classification.
ViViT achieved state-of-the-art results on 5 different public datasets.
Details can be found in the [paper](https://arxiv.org/abs/2103.15691).

## Getting Started
The following command will install the required packages for ViViT:
```shell
$ pip install -r scenic/projects/vivit/requirements.txt
```

ViViT models and training jobs are defined by [configuration files](configs).

To train a model, please download a pretrained ViT image model trained using
[Scenic](https://github.com/google-research/scenic/tree/main/scenic/projects/baselines)
or the [original implementation](https://github.com/google-research/vision_transformer).

Additionally, pre-process the training dataset according to [here](data/data.md).

An example command-line to train ViViT-B/16x2 Factorised Encoder on Kinetics
using this [config file](configs/kinetics400/vivit_base_factorised_encoder.py)
is

```shell
$ python -m scenic.projects.vivit.main \
  --config=scenic/projects/vivit/configs/kinetics400/vivit_base_factorised_encoder.py \
  --workdir=vivit_base_factorised_encoder/
```


## Model Zoo

The following table contains pretrained ViViT models trained on various datasets.
Checkpoints are provided as Scenic checkpoints compatible with
[Flax](https://github.com/google/flax), and also as
[Tensorflow SavedModels](https://www.tensorflow.org/guide/saved_model)
for inference.

"FE" refers to ViViT Factorised Encoder models, as described in the [paper](https://arxiv.org/abs/2103.15691).
Accuracy is reported from "multi-view evaluation", as common in the literature.
In the table below, `x * y` denotes `x` temporal views, and `y` spatial views.
All the models below take in 32 frames as input.

| Model           | Dataset       | Top 1 Accuracy | Views | Config                                                                                                             | Checkpoint                                                                                                                                                                                                                                         |
|:------------:|:-----------:|:------------:|:---:|:----------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| ViViT-B/16x2    | Kinetics 400  | 79.9           | 4x3   | [configs/kinetics400/vivit_base_k400.py](configs/kinetics400/vivit_base_k400.py)                                   | [Checkpoint](https://storage.googleapis.com/scenic-bucket/vivit/kinetics_400/vivit_base_16x2_unfactorized/checkpoint) [SavedModel](https://storage.cloud.google.com/scenic-bucket/vivit/kinetics_400/vivit_base_16x2_unfactorized/saved_model.zip) |
| ViViT-B/16x2 FE | Kinetics 400  | 78.4           | 4x3   | [configs/kinetics400/vivit_base_factorised_encoder.py](configs/kinetics400/vivit_base_factorised_encoder.py)       | [Checkpoint](https://storage.googleapis.com/scenic-bucket/vivit/kinetics_400/vivit_base_16x2_fe/checkpoint) [SavedModel](https://storage.googleapis.com/scenic-bucket/vivit/kinetics_400/vivit_base_16x2_fe/saved_model.zip)                       |
| ViViT-L/16x2 FE | Kinetics 400  | 80.3           | 4x3   | [configs/kinetics400/vivit_large_factorised_encoder.py](configs/kinetics400/vivit_large_factorised_encoder.py)     | [Checkpoint](https://storage.googleapis.com/scenic-bucket/vivit/kinetics_400/vivit_large_16x2_fe/checkpoint) [SavedModel](https://storage.googleapis.com/scenic-bucket/vivit/kinetics_400/vivit_large_16x2_fe/saved_model.zip)                     |
| ViViT-L/16x2 FE | Kinetics 600  | 81.6           | 4x3   | [configs/kinetics600/vivit_large_factorised_encoder.py](configs/kinetics600/vivit_large_factorised_encoder.py)     | [Checkpoint](https://storage.googleapis.com/scenic-bucket/vivit/kinetics_600/vivit_large_16x2_fe/checkpoint) [SavedModel](https://storage.googleapis.com/scenic-bucket/vivit/kinetics_600/vivit_large_16x2_fe/saved_model.zip)                     |
| ViViT-L/16x2 FE | Epic Kitchens | 43.6           | 4x1   | [configs/epic_kitchens/vivit_large_factorised_encoder.py](configs/epic_kitchens/vivit_large_factorised_encoder.py) | [Checkpoint](https://storage.googleapis.com/scenic-bucket/vivit/epic_kitchens/vivit_large_16x2_fe/checkpoint) [SavedModel](https://storage.googleapis.com/scenic-bucket/vivit/epic_kitchens/vivit_large_16x2_fe/saved_model.zip)

## Other Unofficial Implementations

Feel free to share your implementation by contacting the authors or sending a
pull request.

- [Keras](https://keras.io/examples/vision/vivit/) by [Aritra Roy Gosthipaty](https://twitter.com/ariG23498) and [Ayush Thakur](https://twitter.com/ayushthakur0)

## Reference

If you use ViViT, please use the following BibTeX entry.

```
@InProceedings{arnab2021vivit,
  title={ViViT: A Video Vision Transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle={International Conference on Computer Vision (ICCV)},
  year={2021}
}
```



=== File: /home/ndelafuente/scenic/scenic/projects/vivit/train_utils.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Training Utilities for ViViT."""

import functools
from typing import Callable, Dict, List, Optional, Tuple, Union

from absl import logging
from flax import jax_utils
import flax.linen as nn
import jax
from jax.example_libraries.optimizers import clip_grads
import jax.numpy as jnp
import jax.profiler
import matplotlib.pyplot as plt
import ml_collections
import numpy as np
from scenic.dataset_lib import dataset_utils
from scenic.model_lib.base_models import model_utils
from scenic.train_lib_deprecated import optimizers
from scenic.train_lib_deprecated import train_utils
import seaborn as sns

# Aliases for custom types:
Array = Union[jnp.ndarray, np.ndarray]
Batch = Dict[str, jnp.ndarray]
MetricFn = Callable[[jnp.ndarray, Dict[str, jnp.ndarray]],
                    Dict[str, Tuple[float, int]]]
LossFn = Callable[[jnp.ndarray, Batch, Optional[jnp.ndarray]], float]


def to_cpu(array: jnp.ndarray):
  """Transfers array (replicated on multiple hosts) to a single host.

  Args:
    array: Replicated array of shape
      [num_hosts, num_devices, local_batch_size, ...]

  Returns:
    array of shape [global_batch_size, ...] where
      global_batch_size = num_devices * local_batch_size
  """
  return jax.device_get(dataset_utils.unshard(jax_utils.unreplicate(array)))


def train_step(
    train_state: train_utils.TrainState,
    batch: Batch,
    *,
    flax_model: nn.Module,
    learning_rate_fn: Callable[[int], float],
    loss_fn: LossFn,
    metrics_fn: MetricFn,
    config: ml_collections.ConfigDict,
    debug: Optional[bool] = False
) -> Tuple[train_utils.TrainState, Dict[str, Tuple[float, int]], float]:
  """Runs a single step of training.

  Given the state of the training and a batch of data, computes
  the loss and updates the parameters of the model.

  Note that in this code, the buffers of the first (train_state) and second
  (batch) arguments are donated to the computation.

  Args:
    train_state: The state of training including the current
      global_step, model_state, rng, and optimizer. The buffer of this argument
      can be donated to the computation.
    batch: A single batch of data. The buffer of this argument can be donated to
      the computation.
    flax_model: A Flax model.
    learning_rate_fn: learning rate scheduler which give the global_step
      generates the learning rate.
    loss_fn: A loss function that given logits, a batch, and parameters of the
      model calculates the loss.
    metrics_fn: A metrics function that given logits and batch of data,
      calculates the metrics as well as the loss.
    config: Configuration of the experiment.
    debug: Whether the debug mode is enabled during training. `debug=True`
      enables model specific logging/storing some values using
      jax.host_callback.

  Returns:
    Updated state of training, computed metrics, and learning rate for logging.
  """
  new_rng, rng = jax.random.split(train_state.rng)

  if config.get('mixup') and config.mixup.alpha:
    mixup_rng, rng = jax.random.split(rng, 2)
    mixup_rng = train_utils.bind_rng_to_host_device(
        mixup_rng,
        axis_name='batch',
        bind_to=config.mixup.get('bind_to', 'device'))
    batch = dataset_utils.mixup(
        batch,
        config.mixup.alpha,
        config.mixup.get('image_format', 'NTHWC'),
        rng=mixup_rng)

  # Bind the rng to the host/device we are on for dropout.
  dropout_rng = train_utils.bind_rng_to_host_device(
      rng, axis_name='batch', bind_to='device')

  def training_loss_fn(params):
    variables = {'params': params, **train_state.model_state}
    logits, new_model_state = flax_model.apply(
        variables,
        batch['inputs'],
        mutable=['batch_stats'],
        train=True,
        rngs={'dropout': dropout_rng},
        debug=debug)
    loss = loss_fn(logits, batch, variables['params'])
    return loss, (new_model_state, logits)

  compute_gradient_fn = jax.value_and_grad(training_loss_fn, has_aux=True)
  step = train_state.global_step
  lr = learning_rate_fn(step)
  if config.get('sam_rho', None) is None:
    # Normal training
    (train_cost,
     (new_model_state,
      logits)), grad = compute_gradient_fn(train_state.optimizer.target)
  else:
    # SAM training, taken from cl/373487774
    def dual_vector(y: jnp.ndarray) -> jnp.ndarray:
      """Returns the solution of max_x y^T x s.t. ||x||_2 <= 1."""
      gradient_norm = jnp.sqrt(sum(
          [jnp.sum(jnp.square(e)) for e in jax.tree_util.tree_leaves(y)]))
      normalized_gradient = jax.tree_util.tree_map(
          lambda x: x / (gradient_norm + 1e-7), y)
      return normalized_gradient

    g_sam, _ = jax.grad(training_loss_fn, has_aux=True)(
        train_state.optimizer.target)
    g_sam = dual_vector(g_sam)
    target_sam = jax.tree_util.tree_map(
        lambda a, b: a + config.get('sam_rho') * b,
        train_state.optimizer.target, g_sam)
    (train_cost,
     (new_model_state,
      logits)), grad = compute_gradient_fn(target_sam)

  # TODO(dehghani,aarnab): Check how to move this after the pmeam.
  if config.get('max_grad_norm', None) is not None:
    grad = clip_grads(grad, config.max_grad_norm)

  del train_cost
  # Re-use same axis_name as in the call to `pmap(...train_step...)` below.
  grad = jax.lax.pmean(grad, axis_name='batch')
  new_optimizer = train_state.optimizer.apply_gradient(grad, learning_rate=lr)

  # Explicit weight decay, if necessary.
  if config.get('explicit_weight_decay', None) is not None:
    new_optimizer = new_optimizer.replace(
        target=optimizers.tree_map_with_names(
            functools.partial(
                optimizers.decay_weight_fn,
                lr=lr,
                decay=config.explicit_weight_decay),
            new_optimizer.target,
            match_name_fn=lambda name: 'kernel' in name))

  metrics = metrics_fn(logits, batch)
  new_train_state = train_state.replace(  # pytype: disable=attribute-error
      global_step=step + 1,
      optimizer=new_optimizer,
      model_state=new_model_state,
      rng=new_rng)
  return new_train_state, metrics, lr


def eval_step(
    train_state: train_utils.TrainState,
    batch: Batch,
    *,
    flax_model: nn.Module,
    metrics_fn: MetricFn,
    return_logits_and_labels: bool = False,
    return_confusion_matrix: bool = False,
    debug: Optional[bool] = False,
) -> Union[
    Tuple[Dict[str, Tuple[float, int]], jnp.ndarray, jnp.ndarray],
    Tuple[Dict[str, Tuple[float, int]], jnp.ndarray],
    Dict[str, Tuple[float, int]],
]:
  """Runs a single step of training.

  Note that in this code, the buffer of the second argument (batch) is donated
  to the computation.

  Assumed API of metrics_fn is:
  ```metrics = metrics_fn(logits, batch)
  where batch is yielded by the batch iterator, and metrics is a dictionary
  mapping metric name to a vector of per example measurements. eval_step will
  aggregate (by summing) all per example measurements and divide by the
  aggregated normalizers. For each given metric we compute:
  1/N sum_{b in batch_iter} metric(b), where  N is the sum of normalizer
  over all batches.

  Args:
    train_state: TrainState, the state of training including the current
      global_step, model_state, rng, and optimizer. The buffer of this argument
      can be donated to the computation.
    batch: A single batch of data. a metrics function, that given logits and
      batch of data, calculates the metrics as well as the loss.
    flax_model: A Flax model.
    metrics_fn: A metrics function, that given logits and batch of data,
      calculates the metrics as well as the loss.
    return_logits_and_labels: If true, returns logits and labels. Can be used
      for calculating the Mean Average Precision for multi-label problems.
      Only one of "return_logits_and_labels" and "return_confusion_matrix"
      should be true, with the latter taking precedence if both are set as true.
    return_confusion_matrix: If true, returns confusion matrix. Can be used
      to calculate additional metrics for k-way classification problems.
    debug: Whether the debug mode is enabled during evaluation.
      `debug=True` enables model specific logging/storing some values using
      jax.host_callback.

  Returns:
    Calculated metrics [and optionally logits or confusion matrix].
  """
  variables = {
      'params': train_state.optimizer.target,
      **train_state.model_state
  }
  logits = flax_model.apply(
      variables, batch['inputs'], train=False, mutable=False, debug=debug)
  metrics = metrics_fn(logits, batch)

  if return_confusion_matrix:
    confusion_matrix = get_confusion_matrix(
        labels=batch['label'], logits=logits, batch_mask=batch['batch_mask'])
    confusion_matrix = jax.lax.all_gather(confusion_matrix, 'batch')
    return metrics, confusion_matrix

  if return_logits_and_labels:
    logits = jax.lax.all_gather(logits, 'batch')
    labels = jax.lax.all_gather(batch['label'], 'batch')
    return metrics, logits, labels

  return metrics


def test_step(
    train_state: train_utils.TrainState,
    batch: Batch,
    *,
    flax_model: nn.Module,
    metrics_fn: MetricFn,
    n_clips: int = 2,
    return_logits_and_labels: bool = False,
    softmax_logits: bool = False,
    debug: bool = False,
) -> Union[
    Dict[str, Tuple[float, int]],
    Tuple[Dict[str, Tuple[float, int]], jnp.ndarray, jnp.ndarray],
]:
  """Runs a single step of testing.

  For multi-crop testing, we assume that num_crops consecutive entries in the
  batch are from the same example. And we average the logits over these examples

  We assume that the batch contains different crops of the same original
  example. Therefore, we can average all the logits of it.
  This assumption is true when local_batch_size = num_local_devices

  Args:
    train_state: The state of training including the current
      global_step, model_state, rng, and optimizer, and other metadata.
    batch: Dictionary with keys 'inputs', 'labels', 'batch_mask'. We assume that
      all the inputs correspond to the same original example in the test set.
      The input shapes to this function are batch['inputs'] = [num_crops, t, h,
      w, c] batch['labels'] = [num_crops, num_classes] However, for
      classification, the labels for all the crops are the same.
      batch['batch_mask'] = [num_crops]
    flax_model: A Flax model.
    metrics_fn: Metrics function for the model.
    n_clips: The number of clips to process at a time by each device. Set
      due to memory constraints.
    return_logits_and_labels: Whether return logits of the model or not.
    softmax_logits: Whether to softmax-normalise the logits before
      averaging
    debug: Whether the debug mode is enabled during evaluation.
      `debug=True` enables model specific logging/storing some values using
      jax.host_callback.

  Returns:
    Calculated metrics [and optionally averaged logits that are of
    shape `[1, num_classes]`].
  """

  all_logits = jnp.zeros(batch['label'].shape[1])
  assert len(batch['batch_mask'].shape) == 1, (
      'Spatial padding is not supported in multi-crop evaluation.')

  num_crops = batch['inputs'].shape[0]

  variables = {
      'params': train_state.optimizer.target,
      **train_state.model_state
  }
  for idx in range(0, num_crops, n_clips):
    temp_input = batch['inputs'][idx:idx + n_clips]
    logits = flax_model.apply(
        variables, temp_input, train=False, mutable=False, debug=debug)
    if softmax_logits:
      logits = nn.softmax(logits, axis=-1)
    logits = jnp.sum(logits, axis=0)
    all_logits = all_logits + logits

  all_logits = all_logits / num_crops
  all_logits = jnp.expand_dims(all_logits, axis=0)
  batch['label'] = jnp.expand_dims(batch['label'][0], axis=0)
  batch['batch_mask'] = jnp.expand_dims(batch['batch_mask'][0], axis=0)
  metrics = metrics_fn(all_logits, batch)
  if return_logits_and_labels:
    return metrics, all_logits, batch['label']
  return metrics


def get_confusion_matrix(labels: Array, logits: Array,
                         batch_mask: Array) -> Array:
  """Computes confusion matrix from predictions.

  Args:
    labels: [n_batch] or [n_batch, n_classes] array. In the latter case, labels
      are assumed to be one-hot, since the confusion matrix is only defined when
      each example has one label.
    logits: [n_batch, n_classes] array, which are the predictions of the model.
    batch_mask: [n_batch] array. Entries should be 1 or 0, and indicate if the
      example is valid or not.

  Returns:
    confusion_matrix of shape [1, n_classes, n_classes]
  """
  if labels.ndim == logits.ndim:  # one-hot targets
    y_true = jnp.argmax(labels, axis=-1)
  else:
    y_true = labels
  y_pred = jnp.argmax(logits, axis=-1)

  # Prepare sample weights for confusion matrix:
  weights = batch_mask.astype(jnp.float32)

  confusion_matrix = model_utils.confusion_matrix(
      y_true=y_true,
      y_pred=y_pred,
      num_classes=logits.shape[-1],
      weights=weights)
  confusion_matrix = confusion_matrix[jnp.newaxis, ...]  # Dummy batch dim.
  return confusion_matrix


def render_confusion_matrices(confusion_matrices: List[Array],
                              normalization_method: str = 'cols',
                              figsize: Tuple[int, int] = (12, 12),
                              dpi: int = 100,
                              font_scale: int = 3) -> Array:
  """Render confusion matrix so that it can be logged to Tensorboard.

  Args:
    confusion_matrices: List of [n_batch, n_class, n_class] confusion matrices.
      The first two dimensions will be summed over to get an [n_class, n_class]
      matrix for rendering.
    normalization_method: Method of normalizing the confusion matrix before
      plotting. Supported values are one of "cols", "rows" and "none".
      If any other value, no normalization is performed.
    figsize: The figure size used by matplotlib and seaborn.
    dpi: The dpi used by matplotlib and seaborn.
    font_scale: The font scale used by seaborn.

  Returns:
    image: Rendered image of the confusion matrix for plotting. Data type is
      uint8 and values are in range [0, 255]. Shape is
      [1, figsize * dpi, figsize * dpi, 3]
  """
  conf_matrix = np.sum(confusion_matrices, axis=0)  # Sum over eval batches.
  if conf_matrix.ndim != 3:
    raise AssertionError(
        'Expecting confusion matrix to have shape '
        f'[batch_size, num_classes, num_classes], got {conf_matrix.shape}.')
  conf_matrix = np.sum(conf_matrix, axis=0)  # Sum over batch dimension.

  if normalization_method not in {'rows', 'cols', 'none'}:
    logging.warning('Normalizer must be one of {rows, cols, none}.'
                    'Defaulting to none.')

  sns.set(font_scale=font_scale)
  fig = plt.figure(figsize=figsize, dpi=dpi)

  # Normalize entries of the confusion matrix.
  if normalization_method == 'rows':
    normalizer = conf_matrix.sum(axis=1)[:, np.newaxis]
  elif normalization_method == 'cols':
    normalizer = conf_matrix.sum(axis=0)[np.newaxis, :]
  else:
    normalizer = 1
  normalized_matrix = np.nan_to_num(conf_matrix / normalizer)

  if np.sum(normalized_matrix) > 0:
    sns.heatmap(
        normalized_matrix,
        annot=True,
        linewidths=0.5,
        square=True,
        cbar=False,
        cmap='jet',
        annot_kws={'size': 18})
    fig.tight_layout(pad=0.0)

  fig.canvas.draw()
  ncols, nrows = fig.canvas.get_width_height()
  image = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8)
  image = image.reshape(nrows, ncols, 3)
  return np.expand_dims(image, axis=0)



=== File: /home/ndelafuente/scenic/scenic/projects/vivit/__init__.py ===





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/model.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""ViViT: Vision Transformer for Video."""

import functools
from typing import Any, Optional, Callable, Sequence

from absl import logging
import flax.linen as nn
from flax.linen.linear import default_kernel_init
from immutabledict import immutabledict
import jax
import jax.numpy as jnp
import ml_collections
import numpy as np
from scenic.common_lib import video_utils
from scenic.model_lib.base_models import base_model
from scenic.model_lib.base_models import classification_model
from scenic.model_lib.base_models import model_utils as base_model_utils
from scenic.model_lib.base_models.classification_model import ClassificationModel
from scenic.model_lib.layers import attention_layers
from scenic.model_lib.layers import nn_layers
from scenic.projects.baselines import vit
from scenic.projects.vivit import model_utils

Initializer = Callable[[jnp.ndarray, Sequence[int], jnp.dtype], jnp.ndarray]


def get_model_cls(model_name):
  """"Selects Vivit model type."""
  if model_name == 'vivit_multilabel_classification':
    return ViViTMultilabelClassificationModel
  elif model_name == 'vivit_classification':
    return ViViTClassificationModel
  elif model_name == 'vivit_multihead_classification':
    return ViViTMultiHeadClassificationModel
  else:
    raise ValueError('Unrecognized model: {}'.format(model_name))


_AXIS_TO_NAME = immutabledict({
    1: 'time',
    2: 'space',
})

KERNEL_INITIALIZERS = immutabledict({
    'zero': nn.initializers.zeros,
    'xavier': nn.initializers.xavier_uniform(),
})

ViViT_CLASSIFICATION_METRICS_BASIC = immutabledict({
    'accuracy': (base_model_utils.weighted_correctly_classified,
                 base_model_utils.num_examples),
    'loss': (base_model_utils.weighted_unnormalized_softmax_cross_entropy,
             base_model_utils.num_examples)
})

ViViT_CLASSIFICATION_METRICS = immutabledict({
    **ViViT_CLASSIFICATION_METRICS_BASIC,
    'accuracy_top_5': (functools.partial(
        base_model_utils.weighted_topk_correctly_classified,
        k=5), base_model_utils.num_examples),
})


def _reshape_to_time_space(x, temporal_dims):
  if x.ndim == 3:
    b, thw, d = x.shape
    assert thw % temporal_dims == 0
    hw = thw // temporal_dims
    x = jnp.reshape(x, [b, temporal_dims, hw, d])
  assert x.ndim == 4
  return x


def embed_2d_patch(x, patches, embedding_dim):
  """Standard ViT method of embedding input patches."""

  n, h, w, c = x.shape

  assert patches.get('size') is not None, ('patches.size is now the only way'
                                           'to define the patches')

  fh, fw = patches.size
  gh, gw = h // fh, w // fw

  if embedding_dim:
    x = nn.Conv(
        embedding_dim, (fh, fw),
        strides=(fh, fw),
        padding='VALID',
        name='embedding')(x)
  else:
    # This path often results in excessive padding: b/165788633
    x = jnp.reshape(x, [n, gh, fh, gw, fw, c])
    x = jnp.transpose(x, [0, 1, 3, 2, 4, 5])
    x = jnp.reshape(x, [n, gh, gw, -1])

  return x


def embed_3d_patch(x,
                   patches,
                   embedding_dim,
                   kernel_init_method,
                   name='embedding'):
  """Embed 3D input patches into tokens."""

  assert patches.get('size') is not None, 'patches.size must be defined'
  assert len(patches.size) == 3, 'patches.size must have 3 elements'
  assert embedding_dim, 'embedding_dim must be specified'

  fh, fw, ft = patches.size

  if kernel_init_method == 'central_frame_initializer':
    kernel_initializer = model_utils.central_frame_initializer()
    logging.info('Using central frame initializer for input embedding')
  elif kernel_init_method == 'average_frame_initializer':
    kernel_initializer = model_utils.average_frame_initializer()
    logging.info('Using average frame initializer for input embedding')
  else:
    kernel_initializer = default_kernel_init
    logging.info('Using default initializer for input embedding')

  x = nn.Conv(
      embedding_dim, (ft, fh, fw),
      strides=(ft, fh, fw),
      padding='VALID',
      name=name,
      kernel_init=kernel_initializer)(
          x)

  return x


def temporal_encode(x,
                    temporal_encoding_config,
                    patches,
                    hidden_size,
                    return_1d=True,
                    name='embedding'):
  """Encode video for feeding into ViT."""

  n, _, in_h, in_w, c = x.shape

  if temporal_encoding_config.method == 'temporal_sampling':
    n_sampled_frames = temporal_encoding_config.n_sampled_frames
    x = video_utils.sample_frames_uniformly(x, n_sampled_frames)
    t_s = x.shape[1]
    x = jnp.reshape(x, [n, t_s * in_h, in_w, c])

    x = embed_2d_patch(x, patches, hidden_size)
    temporal_dims = t_s
    if return_1d:
      n, th, w, c = x.shape
      x = jnp.reshape(x, [n, th * w, c])
    else:
      n, th, w, c = x.shape
      x = jnp.reshape(x, [n, t_s, -1, w, c])

  elif temporal_encoding_config.method == '3d_conv':
    kernel_init_method = temporal_encoding_config.get('kernel_init_method',
                                                      None)
    x = embed_3d_patch(x, patches, hidden_size, kernel_init_method, name)
    temporal_dims = x.shape[1]
    if return_1d:
      n, t, h, w, c = x.shape
      x = jnp.reshape(x, [n, t * h * w, c])

  else:
    raise AssertionError('Unknown temporal encoding method.')

  assert x.size > 0, ('Found zero tokens after temporal encoding. '
                      'Perhaps one of the patch sizes is such that '
                      'floor(dim_size / patch_size) = 0?')

  return x, temporal_dims


class EncoderBlock(nn.Module):
  """Transformer encoder block.

  Attributes:
    mlp_dim: Dimension of the mlp on top of attention block.
    num_heads: Number of heads.
    attention_axis: Axis over which we run attention.
    dropout_rate: Dropout rate.
    attention_dropout_rate: Dropout for attention heads.
    droplayer_p: Probability of dropping a layer.
    attention_kernel_initializer: Initializer to use for attention
      layers.
    deterministic: Deterministic or not (to apply dropout).
    attention_fn: dot_product_attention or compatible function. Accepts query,
      key, value, and returns output of shape `[bs, dim1, dim2, ..., dimN,,
      num_heads, value_channels]``
    dtype: The dtype of the computation (default: float32).

  Returns:
    Output after transformer encoder block.
  """
  mlp_dim: int
  num_heads: int
  dtype: jnp.dtype = jnp.float32
  dropout_rate: float = 0.1
  attention_dropout_rate: float = 0.1
  attention_kernel_initializer: Initializer = nn.initializers.xavier_uniform()
  attention_fn: Any = nn.dot_product_attention
  droplayer_p: float = 0.0

  def get_drop_pattern(self, x, deterministic):
    if not deterministic and self.droplayer_p:
      shape = (x.shape[0],) + (1,) * (x.ndim - 1)
      return jax.random.bernoulli(
          self.make_rng('dropout'), self.droplayer_p, shape).astype('float32')
    else:
      return 0.0

  @nn.compact
  def __call__(self, inputs: jnp.ndarray, deterministic: bool) -> jnp.ndarray:
    """Applies Encoder1DBlock module."""

    # Attention block.
    x = nn.LayerNorm(dtype=self.dtype)(inputs)
    x = nn.MultiHeadDotProductAttention(
        num_heads=self.num_heads,
        kernel_init=self.attention_kernel_initializer,
        broadcast_dropout=False,
        dropout_rate=self.attention_dropout_rate,
        attention_fn=self.attention_fn,
        dtype=self.dtype)(
            x, x, deterministic=deterministic)
    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic)

    drop_pattern = self.get_drop_pattern(x, deterministic)
    x = x * (1.0 - drop_pattern) + inputs

    # MLP block.
    y = nn.LayerNorm(dtype=self.dtype)(x)
    y = attention_layers.MlpBlock(
        mlp_dim=self.mlp_dim,
        dtype=self.dtype,
        dropout_rate=self.dropout_rate,
        activation_fn=nn.gelu,
        kernel_init=nn.initializers.xavier_uniform(),
        bias_init=nn.initializers.normal(stddev=1e-6))(
            y, deterministic=deterministic)

    drop_pattern = self.get_drop_pattern(x, deterministic)
    return y * (1.0 - drop_pattern) + x


class EncoderFactorizedSelfAttentionBlock(nn.Module):
  """Encoder with facctorized self attention block.

  Attributes:
    mlp_dim: Dimension of the mlp on top of attention block.
    num_heads: Number of heads.
    temporal_dims: Number of temporal dimensions in the flattened input
    attention_kernel_initializer: Initializer to use for attention layers.
    dropout_rate: Dropout rate.
    attention_dropout_rate: Dropout for attention heads.
    droplayer_p: Probability of dropping a layer.
    attention_order: The order to do the attention. Choice of {time_space,
      space_time}.
    dtype: the dtype of the computation (default: float32).
  """
  mlp_dim: int
  num_heads: int
  temporal_dims: int
  attention_kernel_initializer: Initializer
  dropout_rate: float = 0.1
  attention_dropout_rate: float = 0.1
  droplayer_p: Optional[float] = None
  attention_order: str = 'time_space'
  dtype: jnp.dtype = jnp.float32

  @nn.compact
  def __call__(self, inputs: jnp.ndarray, *, deterministic: bool):
    """Applies Encoder1DBlock module."""
    b, thw, d = inputs.shape
    inputs = _reshape_to_time_space(inputs, self.temporal_dims)
    self_attention = functools.partial(
        nn.SelfAttention,
        num_heads=self.num_heads,
        kernel_init=self.attention_kernel_initializer,
        broadcast_dropout=False,
        dropout_rate=self.attention_dropout_rate,
        dtype=self.dtype)

    if self.attention_order == 'time_space':
      attention_axes = (1, 2)
    elif self.attention_order == 'space_time':
      attention_axes = (2, 1)
    else:
      raise ValueError(f'Invalid attention order {self.attention_order}.')

    def _run_attention_on_axis(inputs, axis, two_d_shape):
      """Reshapes the input and run attention on the given axis."""
      inputs = model_utils.reshape_to_1d_factorized(inputs, axis=axis)
      x = nn.LayerNorm(
          dtype=self.dtype, name='LayerNorm_{}'.format(_AXIS_TO_NAME[axis]))(
              inputs)
      x = self_attention(
          name='MultiHeadDotProductAttention_{}'.format(_AXIS_TO_NAME[axis]))(
              x, deterministic=deterministic)
      x = nn.Dropout(rate=self.dropout_rate)(x, deterministic)
      x = x + inputs
      return model_utils.reshape_to_2d_factorized(
          x, axis=axis, two_d_shape=two_d_shape)

    x = inputs
    two_d_shape = inputs.shape
    for axis in attention_axes:
      x = _run_attention_on_axis(x, axis, two_d_shape)

    # MLP block.
    x = jnp.reshape(x, [b, thw, d])
    y = nn.LayerNorm(dtype=self.dtype, name='LayerNorm_mlp')(x)
    y = attention_layers.MlpBlock(
        mlp_dim=self.mlp_dim,
        dtype=self.dtype,
        dropout_rate=self.dropout_rate,
        activation_fn=nn.gelu,
        kernel_init=nn.initializers.xavier_uniform(),
        bias_init=nn.initializers.normal(stddev=1e-6),
        name='MlpBlock')(
            y, deterministic=deterministic)
    return x + y


class Encoder(nn.Module):
  """Transformer Encoder.

  Attributes:
    inputs: nd-array, Input data
    temporal_dims: Number of temporal dimensions in the input.
    mlp_dim: Dimension of the mlp on top of attention block.
    num_layers: Number of layers.
    num_heads: Number of attention heads.
    attention_config: Has parameters for the type of attention.
    dropout_rate: Dropout rate.
    attention_dropout_rate: Dropout for attention heads.
    stochastic_droplayer_rate: Probability of dropping a layer linearly
      grows from 0 to the provided value. Our implementation of stochastic
      depth follows timm library, which does per-example layer dropping and
      uses independent dropping patterns for each skip-connection.
    positional_embedding: The type of positional embedding to use. Supported
      values are {learned_1d, sinusoidal_1d, sinusoidal_3d, none}.
    normalise_output: If True, perform layernorm on the output.
  """

  temporal_dims: Optional[int]
  mlp_dim: int
  num_layers: int
  num_heads: int
  attention_config: ml_collections.ConfigDict = None
  dropout_rate: float = 0.1
  attention_dropout_rate: float = 0.1
  stochastic_droplayer_rate: float = 0.0
  dtype: jnp.dtype = jnp.float32
  positional_embedding: str = 'learned_1d'
  normalise_output: bool = True

  @nn.compact
  def __call__(self, inputs: jnp.ndarray, *, train: bool):
    """Applies Transformer model on the inputs."""
    assert inputs.ndim == 3  # (batch, len, emb)
    dtype = jax.dtypes.canonicalize_dtype(self.dtype)

    if self.positional_embedding == 'learned_1d':
      x = vit.AddPositionEmbs(
          posemb_init=nn.initializers.normal(stddev=0.02),  # from BERT.
          name='posembed_input')(inputs)
    elif self.positional_embedding == 'sinusoidal_1d':
      x = attention_layers.Add1DPositionEmbedding(
          posemb_init=None)(inputs)
    elif self.positional_embedding == 'sinusoidal_3d':
      batch, num_tokens, hidden_dim = inputs.shape
      height = width = int(np.sqrt(num_tokens // self.temporal_dims))
      if height * width * self.temporal_dims != num_tokens:
        raise ValueError('Input is assumed to be square for sinusoidal init.')
      inputs_reshape = inputs.reshape([batch, self.temporal_dims, height, width,
                                       hidden_dim])
      x = attention_layers.AddFixedSinCosPositionEmbedding()(inputs_reshape)
      x = x.reshape([batch, num_tokens, hidden_dim])
    elif self.positional_embedding == 'none':
      x = inputs
    else:
      raise ValueError(
          f'Unknown positional embedding {self.positional_embedding}')
    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=not train)

    if self.attention_config is None or self.attention_config.type in [
        'spacetime', 'factorized_encoder'
    ]:
      encoder_block = EncoderBlock
    elif self.attention_config.type == 'factorized_self_attention_block':
      encoder_block = functools.partial(
          EncoderFactorizedSelfAttentionBlock,
          attention_order=self.attention_config.attention_order,
          attention_kernel_initializer=KERNEL_INITIALIZERS[
              self.attention_config.get('attention_kernel_init_method',
                                        'xavier')],
          temporal_dims=self.temporal_dims)
    elif self.attention_config.type == 'factorized_dot_product_attention':
      b, thw, d = x.shape
      x = _reshape_to_time_space(x, self.temporal_dims)  # [b, t, hw, d]
      encoder_block = functools.partial(
          EncoderBlock,
          attention_fn=functools.partial(
              model_utils.factorized_dot_product_attention))
    else:
      raise ValueError(f'Unknown attention type {self.attention_config.type}')

    # Input Encoder
    for lyr in range(self.num_layers):
      droplayer_p = (
          lyr / max(self.num_layers - 1, 1)) * self.stochastic_droplayer_rate
      x = encoder_block(
          mlp_dim=self.mlp_dim,
          num_heads=self.num_heads,
          dropout_rate=self.dropout_rate,
          attention_dropout_rate=self.attention_dropout_rate,
          droplayer_p=droplayer_p,
          name=f'encoderblock_{lyr}',
          dtype=dtype)(
              x, deterministic=not train)

    if self.attention_config.type == 'factorized_dot_product_attention':
      # Reshape back to 3D:
      x = jnp.reshape(x, [b, thw, d])

    if self.normalise_output:
      encoded = nn.LayerNorm(name='encoder_norm')(x)
    else:
      encoded = x

    return encoded


class ViViT(nn.Module):
  """Vision Transformer model for Video.

  Attributes:
    mlp_dim: Dimension of the mlp on top of attention block.
    num_classes: Number of output classes.
    num_heads: Number of self-attention heads.
    num_layers: Number of layers.
    patches: Configuration of the patches extracted in the stem of the model.
    hidden_size: Size of the hidden state of the output of model's stem.
    representation_size: Size of the representation layer in the model's head.
      if None, we skip the extra projection + tanh activation at the end.
    temporal_encoding_config: ConfigDict which defines the type of input
      encoding when tokenising the video.
    attention_config: ConfigDict which defines the type of spatio-temporal
      attention applied in the model.
    dropout_rate: Dropout rate.
    attention_dropout_rate: Dropout for attention heads.
    stochastic_droplayer_rate: Probability of dropping a layer. Linearly
      increases from 0 to the provided value..
    classifier: type of the classifier layer. Options are 'gap', 'gmp', 'gsp',
      'token'.
    return_prelogits: If true, return the final representation of the network
      before the classification head. Useful when using features for a
      downstream task.
    return_preclassifier: If true, return the representation after the
      transformer encoder. Useful if using this as the backbone stem as part
      of a bigger architecture.
    dtype: JAX data type for activations.
  """

  mlp_dim: int
  num_layers: int
  num_heads: int
  num_classes: int
  patches: ml_collections.ConfigDict
  hidden_size: int
  temporal_encoding_config: ml_collections.ConfigDict
  attention_config: ml_collections.ConfigDict
  representation_size: Optional[int] = None
  dropout_rate: float = 0.1
  attention_dropout_rate: float = 0.1
  stochastic_droplayer_rate: float = 0.
  classifier: str = 'gap'
  return_prelogits: bool = False
  return_preclassifier: bool = False
  dtype: jnp.dtype = jnp.float32

  @nn.compact
  def __call__(self, x: jnp.ndarray, *, train: bool, debug: bool = False):

    assert self.classifier in ['token', '0', 'gap', 'gmp', 'gsp']
    attention_type = self.attention_config.get('type', 'spacetime')
    if attention_type in [
        'factorized_transformer_block', 'factorized_self_attention_block',
        'factorized_dot_product_attention'
    ]:
      assert self.classifier not in ['token', '0'], (
          'For factorized_transformer_block, factorized_self_attention_block'
          'and factorized_dot_product_attention, the token classifier is not'
          'implemented.')

    x, temporal_dims = temporal_encode(
        x, self.temporal_encoding_config, self.patches, self.hidden_size)

    # If we want to add a class token, add it here.
    if self.classifier in ['token']:
      n, _, c = x.shape
      cls = self.param('cls', nn.initializers.zeros, (1, 1, c), x.dtype)
      cls = jnp.tile(cls, [n, 1, 1])
      x = jnp.concatenate([cls, x], axis=1)

    x = Encoder(
        temporal_dims=temporal_dims,
        mlp_dim=self.mlp_dim,
        num_layers=self.num_layers,
        num_heads=self.num_heads,
        attention_config=self.attention_config,
        dropout_rate=self.dropout_rate,
        attention_dropout_rate=self.attention_dropout_rate,
        stochastic_droplayer_rate=self.stochastic_droplayer_rate,
        dtype=self.dtype,
        name='Transformer')(
            x, train=train)

    if self.return_preclassifier:
      return x

    if self.classifier in ['token', '0']:
      x = x[:, 0]
    elif self.classifier in ('gap', 'gmp', 'gsp'):
      fn = {'gap': jnp.mean, 'gmp': jnp.max, 'gsp': jnp.sum}[self.classifier]
      x = fn(x, axis=list(range(1, x.ndim - 1)))

    if self.representation_size is not None:
      x = nn.Dense(self.representation_size, name='pre_logits')(x)
      x = nn.tanh(x)
    else:
      x = nn_layers.IdentityLayer(name='pre_logits')(x)

    if self.return_prelogits:
      return x
    else:
      x = nn.Dense(
          self.num_classes,
          kernel_init=nn.initializers.zeros,
          name='output_projection')(x)
      return x


class SpaceTimeViViT(nn.Module):
  """ViT model for Video with factorized space-time attention."""

  spatial_mlp_dim: int
  spatial_num_layers: int
  spatial_num_heads: int
  temporal_mlp_dim: int
  temporal_num_layers: int
  temporal_num_heads: int
  num_classes: int
  patches: ml_collections.ConfigDict
  hidden_size: int
  temporal_encoding_config: ml_collections.ConfigDict
  attention_config: ml_collections.ConfigDict
  representation_size: Optional[int] = None
  dropout_rate: float = 0.1
  attention_dropout_rate: float = 0.1
  stochastic_droplayer_rate: float = 0.
  classifier: str = 'gap'
  return_prelogits: bool = False
  dtype: jnp.dtype = jnp.float32

  @nn.compact
  def __call__(self, x: jnp.ndarray, *, train: bool, debug: bool = False):

    del debug
    x, _ = temporal_encode(
        x, self.temporal_encoding_config, self.patches, self.hidden_size,
        return_1d=False)
    bs, t, h, w, c = x.shape
    x = x.reshape(bs, t, h * w, c)

    def vit_body(x, mlp_dim, num_layers, num_heads, encoder_name='Transformer'):
      # If we want to add a class token, add it here.
      if self.classifier in ['token']:
        n, _, c = x.shape
        cls = self.param(f'cls_{encoder_name}', nn.initializers.zeros,
                         (1, 1, c), x.dtype)
        cls = jnp.tile(cls, [n, 1, 1])
        x = jnp.concatenate([cls, x], axis=1)

      x = Encoder(
          temporal_dims=None,  # This is unused for Factorised-Encoder
          mlp_dim=mlp_dim,
          num_layers=num_layers,
          num_heads=num_heads,
          attention_config=self.attention_config,
          dropout_rate=self.dropout_rate,
          attention_dropout_rate=self.attention_dropout_rate,
          stochastic_droplayer_rate=self.stochastic_droplayer_rate,
          dtype=self.dtype,
          name=encoder_name)(x, train=train)

      if self.classifier in ['token', '0']:
        x = x[:, 0]
      elif self.classifier in ('gap', 'gmp', 'gsp'):
        fn = {'gap': jnp.mean, 'gmp': jnp.max, 'gsp': jnp.sum}[self.classifier]
        x = fn(x, axis=list(range(1, x.ndim - 1)))
      return x

    # run attention across spacec, per frame
    x = jax.vmap(
        functools.partial(
            vit_body,
            mlp_dim=self.spatial_mlp_dim,
            num_layers=self.spatial_num_layers,
            num_heads=self.spatial_num_heads,
            encoder_name='SpatialTransformer'),
        in_axes=1,
        out_axes=1,
        axis_name='time')(
            x)
    assert x.ndim == 3 and x.shape[:2] == (bs, t)

    # run attention across time, over all frames
    if not self.attention_config.get('spatial_only_baseline', False):
      x = vit_body(
          x,
          mlp_dim=self.temporal_mlp_dim,
          num_layers=self.temporal_num_layers,
          num_heads=self.temporal_num_heads,
          encoder_name='TemporalTransformer')
    else:
      # Do global average pooling instead, as method of combining temporal info.
      x = jnp.mean(x, axis=list(range(1, x.ndim - 1)))

    if self.representation_size is not None:
      x = nn.Dense(self.representation_size, name='pre_logits')(x)
      x = nn.tanh(x)
    else:
      x = nn_layers.IdentityLayer(name='pre_logits')(x)

    if self.return_prelogits:
      return x
    else:
      x = nn.Dense(
          self.num_classes,
          kernel_init=nn.initializers.zeros,
          name='output_projection')(x)
      return x


class ViViTClassificationModel(ClassificationModel):
  """Video Transformer model for n-way classification."""

  def build_flax_model(self) -> nn.Module:
    model_dtype = getattr(jnp, self.config.get('model_dtype_str', 'float32'))
    attention_type = self.config.model.attention_config.get(
        'type', 'spacetime')
    if attention_type in [
        'spacetime', 'factorized_transformer_block',
        'factorized_self_attention_block', 'factorized_dot_product_attention'
    ]:
      return ViViT(
          num_classes=self.dataset_meta_data['num_classes'],
          mlp_dim=self.config.model.mlp_dim,
          num_layers=self.config.model.num_layers,
          num_heads=self.config.model.num_heads,
          representation_size=self.config.model.representation_size,
          patches=self.config.model.patches,
          hidden_size=self.config.model.hidden_size,
          temporal_encoding_config=self.config.model.temporal_encoding_config,
          attention_config=self.config.model.attention_config,
          classifier=self.config.model.classifier,
          dropout_rate=self.config.model.get('dropout_rate', 0.1),
          attention_dropout_rate=self.config.model.get(
              'attention_dropout_rate', 0.1),
          stochastic_droplayer_rate=self.config.model.get(
              'stochastic_droplayer_rate', 0),
          return_prelogits=self.config.model.get('return_prelogits', False),
          return_preclassifier=self.config.model.get(
              'return_preclassifier', False),
          dtype=model_dtype,
      )
    elif attention_type == 'factorized_encoder':
      # TODO(dehghani): Rewrite this as a type of attention in ViViT Encoder.
      return SpaceTimeViViT(
          num_classes=self.dataset_meta_data['num_classes'],
          spatial_mlp_dim=self.config.model.spatial_transformer.mlp_dim,
          spatial_num_layers=self.config.model.spatial_transformer.num_layers,
          spatial_num_heads=self.config.model.spatial_transformer.num_heads,
          temporal_mlp_dim=self.config.model.temporal_transformer.mlp_dim,
          temporal_num_layers=self.config.model.temporal_transformer
          .num_layers,
          temporal_num_heads=self.config.model.temporal_transformer.num_heads,
          representation_size=self.config.model.representation_size,
          patches=self.config.model.patches,
          hidden_size=self.config.model.hidden_size,
          temporal_encoding_config=self.config.model.temporal_encoding_config,
          attention_config=self.config.model.attention_config,
          classifier=self.config.model.classifier,
          dropout_rate=self.config.model.get('dropout_rate', 0.1),
          attention_dropout_rate=self.config.model.get(
              'attention_dropout_rate', 0.1),
          stochastic_droplayer_rate=self.config.model.get(
              'stochastic_droplayer_rate', 0),
          return_prelogits=self.config.model.get('return_prelogits', False),
          dtype=model_dtype,
      )
    else:
      raise ValueError(f'Attention type {attention_type} does not exist.')

  def get_metrics_fn(self, split: Optional[str] = None) -> base_model.MetricFn:
    """Returns a callable metric function for the model.

    Args:
      split: The split for which we calculate the metrics. It should be one
        of the ['train',  'validation', 'test'].
    Returns: A metric function with the following API: ```metrics_fn(logits,
      label, weights)```
    """
    del split  # for all splits, we return the same metric functions

    metrics = ViViT_CLASSIFICATION_METRICS
    if self.dataset_meta_data.get('num_classes', -1) <= 5:
      metrics = ViViT_CLASSIFICATION_METRICS_BASIC
    return functools.partial(
        classification_model.classification_metrics_function,
        target_is_onehot=self.dataset_meta_data.get('target_is_onehot', False),
        metrics=metrics)

  def init_from_train_state(self,
                            train_state: Any,
                            restored_train_state: Any,
                            restored_model_cfg: ml_collections.ConfigDict,
                            restore_output_proj: bool = False) -> Any:
    """Updates the train_state with data from restored_train_state."""
    attention_type = self.config.model.attention_config.get(
        'type', 'spacetime')
    if attention_type in [
        'spacetime', 'factorized_transformer_block',
        'factorized_self_attention_block', 'factorized_dot_product_attention'
    ]:
      vivit_transformer_key = 'Transformer'
    elif attention_type == 'factorized_encoder':
      vivit_transformer_key = 'SpatialTransformer'
    else:
      raise ValueError(f'Attention type {attention_type} does not exist.')
    return model_utils.initialise_from_train_state(
        self.config,
        train_state,
        restored_train_state,
        restored_model_cfg,
        restore_output_proj,
        vivit_transformer_key=vivit_transformer_key)


class ViViTMultilabelClassificationModel(vit.ViTMultiLabelClassificationModel):
  """Video Transformer model for multi-class classification."""

  def build_flax_model(self) -> nn.Module:
    model_dtype = getattr(jnp, self.config.get('model_dtype_str', 'float32'))
    attention_type = self.config.model.attention_config.get(
        'type', 'spacetime')
    if attention_type in [
        'spacetime', 'factorized_transformer_block',
        'factorized_self_attention_block', 'factorized_dot_product_attention'
    ]:
      return ViViT(
          num_classes=self.dataset_meta_data['num_classes'],
          mlp_dim=self.config.model.mlp_dim,
          num_layers=self.config.model.num_layers,
          num_heads=self.config.model.num_heads,
          representation_size=self.config.model.representation_size,
          patches=self.config.model.patches,
          hidden_size=self.config.model.hidden_size,
          temporal_encoding_config=self.config.model.temporal_encoding_config,
          attention_config=self.config.model.attention_config,
          classifier=self.config.model.classifier,
          dropout_rate=self.config.model.get('dropout_rate', 0.1),
          attention_dropout_rate=self.config.model.get(
              'attention_dropout_rate', 0.1),
          stochastic_droplayer_rate=self.config.model.get(
              'stochastic_droplayer_rate', 0),
          return_prelogits=self.config.model.get('return_prelogits', False),
          return_preclassifier=self.config.model.get(
              'return_preclassifier', False),
          dtype=model_dtype,
      )
    elif attention_type == 'factorized_encoder':
      # TODO(dehghani): Rewrite this as a type of attention in ViViT Encoder.
      return SpaceTimeViViT(
          num_classes=self.dataset_meta_data['num_classes'],
          spatial_mlp_dim=self.config.model.spatial_transformer.mlp_dim,
          spatial_num_layers=self.config.model.spatial_transformer.num_layers,
          spatial_num_heads=self.config.model.spatial_transformer.num_heads,
          temporal_mlp_dim=self.config.model.temporal_transformer.mlp_dim,
          temporal_num_layers=self.config.model.temporal_transformer
          .num_layers,
          temporal_num_heads=self.config.model.temporal_transformer.num_heads,
          representation_size=self.config.model.representation_size,
          patches=self.config.model.patches,
          hidden_size=self.config.model.hidden_size,
          temporal_encoding_config=self.config.model.temporal_encoding_config,
          attention_config=self.config.model.attention_config,
          classifier=self.config.model.classifier,
          dropout_rate=self.config.model.get('dropout_rate', 0.1),
          attention_dropout_rate=self.config.model.get(
              'attention_dropout_rate', 0.1),
          stochastic_droplayer_rate=self.config.model.get(
              'stochastic_droplayer_rate', 0),
          return_prelogits=self.config.model.get('return_prelogits', False),
          dtype=model_dtype,
      )
    else:
      raise ValueError(f'Attention type {attention_type} does not exist.')

  def init_from_train_state(self,
                            train_state: Any,
                            restored_train_state: Any,
                            restored_model_cfg: ml_collections.ConfigDict,
                            restore_output_proj: bool = False) -> Any:
    """Updates the train_state with data from restored_train_state."""
    attention_type = self.config.model.attention_config.get(
        'type', 'spacetime')
    if attention_type in [
        'spacetime', 'factorized_transformer_block',
        'factorized_self_attention_block', 'factorized_dot_product_attention'
    ]:
      vivit_transformer_key = 'Transformer'
    elif attention_type == 'factorized_encoder':
      vivit_transformer_key = 'SpatialTransformer'
    else:
      raise ValueError(f'Attention type {attention_type} does not exist.')
    return model_utils.initialise_from_train_state(
        self.config,
        train_state,
        restored_train_state,
        restored_model_cfg,
        restore_output_proj,
        vivit_transformer_key=vivit_transformer_key)


class ViViTMultiHeadClassificationModel(ViViTClassificationModel):
  """Video Transformer model for multiple n-way classification."""

  def __init__(self, config, dataset_meta_data):
    super().__init__(config, dataset_meta_data)

    assert self.config.dataset_configs.get('class_splits'), (
        'dataset_configs.class_splits must be specified')
    self.class_splits = np.cumsum(self.config.dataset_configs.class_splits)
    if self.config.dataset_configs.get('split_names'):
      self.split_names = self.config.dataset_configs.split_names
    else:
      self.split_names = [str(x + 1) for x in range(len(self.class_splits))]

    assert not config.get('multicrop_softmax_logits', False), (
        'Returning softmaxed logits during multicrop evaluation is not '
        'supported for this model.')

  def loss_function(self,
                    logits: jnp.ndarray,
                    batch: base_model.Batch,
                    model_params: Optional[jnp.ndarray] = None) -> float:
    """Return softmax cross entropy loss with an L2 penalty on the weights."""
    weights = batch.get('batch_mask')

    if self.dataset_meta_data.get('target_is_onehot', False):
      one_hot_targets = batch['label']
    else:
      raise ValueError('Target labels should be one-hot.')

    if logits.shape[-1] != self.class_splits[-1]:
      raise AssertionError('Logit dimension must be equal to number of classes')

    logit_splits = jnp.split(logits, self.class_splits, axis=-1)[:-1]
    one_hot_target_splits = jnp.split(
        one_hot_targets, self.class_splits, axis=-1)[:-1]
    label_smoothing = self.config.get('label_smoothing')

    sof_ce_losses = [
        base_model_utils.weighted_softmax_cross_entropy(
            logits, one_hot_targets, weights, label_smoothing)
        for logits, one_hot_targets in zip(logit_splits, one_hot_target_splits)
    ]
    sof_ce_loss = jnp.mean(jnp.array(sof_ce_losses))

    if self.config.get('l2_decay_factor') is None:
      total_loss = sof_ce_loss
    else:
      l2_loss = base_model_utils.l2_regularization(model_params)
      total_loss = sof_ce_loss + 0.5 * self.config.l2_decay_factor * l2_loss
    return total_loss

  def get_metrics_fn(self, split: Optional[str] = None) -> base_model.MetricFn:
    """Returns a callable metric function for the model.

    Args:
      split: The split for which we calculate the metrics. It should be one
        of the ['train',  'validation', 'test'].
    Returns: A metric function with the following API: ```metrics_fn(logits,
      label, weights)```
    """
    del split  # for all splits, we return the same metric functions

    num_classes_in_each_head = (
        self.dataset_meta_data.get('class_splits', [-1]))
    minimal_num_classes = min(num_classes_in_each_head)
    def classification_metrics_function(logits, batch, metrics, class_splits,
                                        split_names):

      one_hot_targets = batch['label']
      weights = batch.get('batch_mask')  # batch_mask might not be defined

      logit_splits = jnp.split(logits, class_splits, axis=-1)[:-1]
      one_hot_target_splits = jnp.split(
          one_hot_targets, class_splits, axis=-1)[:-1]

      evaluated_metrics = {}
      total_loss = [0.0, 0.0]
      for logits_i, one_hot_targets_i, name in zip(logit_splits,
                                                   one_hot_target_splits,
                                                   split_names):
        for key, val in metrics.items():
          evaluated_metrics[
              f'{name}_{key}'] = base_model_utils.psum_metric_normalizer(
                  (val[0](logits_i, one_hot_targets_i,
                          weights), val[1](logits_i, one_hot_targets_i,
                                           weights)))
          if key == 'loss':
            total_loss[0] += evaluated_metrics[f'{name}_{key}'][0]
            total_loss[1] += evaluated_metrics[f'{name}_{key}'][1]
      evaluated_metrics['total_loss'] = total_loss

      if len(class_splits) == 2:
        pairwise_acc = base_model_utils.psum_metric_normalizer(
            (model_utils.joint_accuracy(logits, one_hot_targets, class_splits,
                                        weights),
             base_model_utils.num_examples(logits, one_hot_targets, weights)))
        eval_name = f'{split_names[0]}-{split_names[1]}'
        evaluated_metrics[f'{eval_name}_accuracy'] = pairwise_acc
        if minimal_num_classes > 5:
          pairwise_top_five = base_model_utils.psum_metric_normalizer(
              (model_utils.joint_top_k(
                  logits, one_hot_targets, class_splits, k=5, weights=weights),
               base_model_utils.num_examples(logits, one_hot_targets, weights)))
          evaluated_metrics[f'{eval_name}_accuracy_top_5'] = pairwise_top_five

      return evaluated_metrics
    metrics = ViViT_CLASSIFICATION_METRICS
    if minimal_num_classes <= 5:
      metrics = ViViT_CLASSIFICATION_METRICS_BASIC
    return functools.partial(
        classification_metrics_function,
        metrics=metrics,
        class_splits=self.class_splits,
        split_names=self.split_names)



=== File: /home/ndelafuente/scenic/scenic/projects/vivit/main.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Main file for ViViT."""

from typing import Any, Callable

from absl import flags
from clu import metric_writers
import jax
import jax.numpy as jnp
import ml_collections
from scenic import app
from scenic.projects.vivit import model as vivit_model
from scenic.projects.vivit import trainer as vivit_trainer
from scenic.train_lib_deprecated import train_utils

FLAGS = flags.FLAGS


def get_trainer(trainer_name: str) -> Callable[..., Any]:
  """Returns trainer given its name."""
  if trainer_name == 'vivit_trainer':
    return vivit_trainer.train
  raise ValueError(f'Unsupported trainer: {trainer_name}.')


def main(rng: jnp.ndarray, config: ml_collections.ConfigDict, workdir: str,
         writer: metric_writers.MetricWriter):
  """Main function for the ViViT project."""
  model_cls = vivit_model.get_model_cls(config.model_name)
  data_rng, rng = jax.random.split(rng)
  dataset = train_utils.get_dataset(
      config, data_rng, dataset_service_address=FLAGS.dataset_service_address)
  trainer = get_trainer(config.trainer_name)

  trainer(
      rng=rng,
      config=config,
      model_cls=model_cls,
      dataset=dataset,
      workdir=workdir,
      writer=writer)


if __name__ == '__main__':
  app.run(main=main)



=== File: /home/ndelafuente/scenic/scenic/projects/vivit/configs/__init__.py ===





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/configs/something_something_v2/vivit_large_factorised_encoder.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

r"""ViViT-Large Factorised Encoder Something-Something v2.

"""


from absl import logging
import ml_collections

SSV2_TRAIN_SIZE = 168913
SSV2_VAL_SIZE = 24777


def get_config():
  """Returns the base experiment configuration."""
  config = ml_collections.ConfigDict()
  config.experiment_name = 'vivit_large_fe_ssv2'

  # Dataset.
  config.dataset_name = 'video_tfrecord_dataset'
  config.dataset_configs = ml_collections.ConfigDict()
  config.data_dtype_str = 'float32'

  config.datset_configs = ml_collections.ConfigDict()
  # Dataset.
  config.dataset_configs.base_dir = 'path/to/dataset/root'
  config.dataset_configs.tables = {
      'train': 'something-something-v2-train.rgb.tfrecord@128',
      'validation': 'something-something-v2-validation.rgb.tfrecord@128',
      'test': 'something-something-v2-validation.rgb.tfrecord@128'
  }
  config.dataset_configs.examples_per_subset = {
      'train': SSV2_TRAIN_SIZE,
      'validation': SSV2_VAL_SIZE,
      'test': SSV2_VAL_SIZE
  }
  config.dataset_configs.num_classes = 174

  # This is going to sample 32 frames, sampled at a stride of 1 from the video.
  # And then it will uniformly take n_sampled_frames from there.
  config.dataset_configs.num_frames = 32
  config.dataset_configs.stride = 1
  config.dataset_configs.min_resize = 288
  config.dataset_configs.crop_size = 256
  config.dataset_configs.one_hot_labels = True
  config.dataset_configs.zero_centering = True

  # Multicrop evaluation settings:
  config.dataset_configs.do_multicrop_test = True  # Do during training.
  config.dataset_configs.log_test_epochs = 5
  # The effective batch size per host when testing is
  # num_test_clips * test_batch_size.
  config.dataset_configs.num_test_clips = 4
  config.dataset_configs.test_batch_size = 8  # Must equal num_local_devices.
  # To take three spatial crops when testing.
  config.dataset_configs.do_three_spatial_crops = True
  config.multicrop_clips_per_device = 2

  # Data augmentation
  config.dataset_configs.augmentation_params = ml_collections.ConfigDict()
  config.dataset_configs.augmentation_params.do_jitter_scale = True
  config.dataset_configs.augmentation_params.scale_min_factor = 0.9
  config.dataset_configs.augmentation_params.scale_max_factor = 1.33
  config.dataset_configs.augmentation_params.prob_scale_jitter = 1.0
  config.dataset_configs.augmentation_params.do_color_augment = False
  config.dataset_configs.augmentation_params.prob_color_augment = 0.8
  config.dataset_configs.augmentation_params.prob_color_drop = 0.1

  # This does Mixup in the train loop. This is fast. But make sure that device
  # batch size is more than 1.
  config.mixup = ml_collections.ConfigDict()
  config.mixup.alpha = 0.3

  config.dataset_configs.augmentation_params.do_rand_augment = True
  config.dataset_configs.augmentation_params.rand_augment_num_layers = 2
  config.dataset_configs.augmentation_params.rand_augment_magnitude = 20

  config.dataset_configs.prefetch_to_device = 2

  # Model: ViViT-Large Factorized Encoder.
  config.model_name = 'vivit_classification'
  config.model = ml_collections.ConfigDict()
  config.model.hidden_size = 1024
  config.model.patches = ml_collections.ConfigDict()
  config.model.patches.size = [16, 16]

  config.model.attention_config = ml_collections.ConfigDict()
  config.model.attention_config.type = 'factorized_encoder'

  config.model.spatial_transformer = ml_collections.ConfigDict()
  config.model.spatial_transformer.num_heads = 16
  config.model.spatial_transformer.mlp_dim = 4096
  config.model.spatial_transformer.num_layers = 24
  config.model.temporal_transformer = ml_collections.ConfigDict()
  config.model.temporal_transformer.num_heads = 16
  config.model.temporal_transformer.mlp_dim = 4096
  config.model.temporal_transformer.num_layers = 4

  config.model.representation_size = None
  config.model.classifier = 'token'
  config.model.attention_dropout_rate = 0.
  config.model.dropout_rate = 0.
  config.model.stochastic_droplayer_rate = 0.3
  config.model_dtype_str = 'float32'

  config.model.temporal_encoding_config = ml_collections.ConfigDict()
  config.model.temporal_encoding_config.method = '3d_conv'
  config.model.patches.size = [16, 16, 2]
  config.model.temporal_encoding_config.kernel_init_method = 'central_frame_initializer'

  # Training.
  config.trainer_name = 'vivit_trainer'
  config.optimizer = 'momentum'
  config.optimizer_configs = ml_collections.ConfigDict()
  config.l2_decay_factor = 0
  config.max_grad_norm = 1
  config.label_smoothing = 0.3
  config.num_training_epochs = 35
  config.batch_size = 64
  config.rng_seed = 0

  # Initialisation.
  config.init_from = ml_collections.ConfigDict()
  # Use a pretrained Kinetics checkpoint, or ImageNet checkpoint
  config.init_from.checkpoint_path = 'path_to_checkpoint'
  config.init_from.checkpoint_format = 'scenic'
  config.init_from.model_config = ml_collections.ConfigDict()
  config.init_from.model_config.model = ml_collections.ConfigDict()
  config.init_from.model_config.model.classifier = 'token'  # Specify if this is 'token' or 'gap'.  pylint: disable=line-too-long
  config.init_from.checkpoint_path = None
  config.init_from.restore_positional_embedding = True
  config.init_from.restore_input_embedding = True
  config.init_from.positional_embed_size_change = 'resize'

  # Learning rate.
  steps_per_epoch = SSV2_TRAIN_SIZE // config.batch_size
  total_steps = config.num_training_epochs * steps_per_epoch
  config.lr_configs = ml_collections.ConfigDict()
  config.lr_configs.learning_rate_schedule = 'compound'
  config.lr_configs.factors = 'constant * cosine_decay * linear_warmup'
  config.lr_configs.warmup_steps = int(2.5 * steps_per_epoch)
  config.lr_configs.steps_per_cycle = total_steps
  config.lr_configs.base_learning_rate = 0.4

  # Logging.
  config.write_summary = True
  config.checkpoint = True  # Do checkpointing.
  config.debug_train = False  # Debug mode during training.
  config.debug_eval = False  # Debug mode during eval.
  config.checkpoint_steps = 500  # Checkpoint more frequently than a val epoch
  config.log_summary_steps = 100

  return config





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/configs/something_something_v2/__init__.py ===





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/configs/epic_kitchens/vivit_large_factorised_encoder.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

r"""ViViT Factorised Encoder model for Epic Kitchens.

"""


from absl import logging
import ml_collections

EPIC_TRAIN_SIZE = 67217
EPIC_VALID_SIZE = 9668


def get_config():
  """Returns the base experiment configuration."""
  config = ml_collections.ConfigDict()
  config.experiment_name = 'vivit_large_factorised_encoder_ek'

  # Dataset.
  config.dataset_name = 'video_tfrecord_dataset'
  config.dataset_configs = ml_collections.ConfigDict()
  config.data_dtype_str = 'float32'
  config.datset_configs = ml_collections.ConfigDict()
  config.dataset_configs.base_dir = (
      '/path/to/dataset')
  config.dataset_configs.tables = {
      'train': 'train.tfrecord@1024',
      'validation': 'validation.tfrecord@1024',
      'test': 'test.tfrecord@1024'
  }
  config.dataset_configs.examples_per_subset = {
      'train': EPIC_TRAIN_SIZE,
      'validation': EPIC_VALID_SIZE,
      'test': EPIC_VALID_SIZE
  }

  ## EpicKitchens-specific flags.
  # We want to train a "multi-head" classification model where we are predicting
  # both "nouns" and "verbs"
  # Setting this as None / leaving it empty, means we concatenate the two labels
  config.dataset_configs.class_splits = [300, 97]
  config.dataset_configs.split_names = ['noun', 'verb']

  # This is going to sample 32 frames, sampled at a stride of 1 from the video.
  config.dataset_configs.num_frames = 32
  config.dataset_configs.stride = 1
  config.dataset_configs.min_resize = 256
  config.dataset_configs.crop_size = 224
  config.dataset_configs.one_hot_labels = True
  config.dataset_configs.zero_centering = True

  # Multicrop evaluation settings:
  config.dataset_configs.do_multicrop_test = True  # Do during training.
  config.dataset_configs.log_test_epochs = 5
  # The effective batch size per host when testing is
  # num_test_clips * test_batch_size.
  config.dataset_configs.num_test_clips = 4
  config.dataset_configs.test_batch_size = 8  # Must equal num_local_devices.
  # To take three spatial crops when testing.
  config.dataset_configs.do_three_spatial_crops = True
  config.multicrop_clips_per_device = 2

  # Data augmentation
  config.dataset_configs.augmentation_params = ml_collections.ConfigDict()
  config.dataset_configs.augmentation_params.do_jitter_scale = True
  config.dataset_configs.augmentation_params.scale_min_factor = 0.9
  config.dataset_configs.augmentation_params.scale_max_factor = 1.33
  config.dataset_configs.augmentation_params.prob_scale_jitter = 1.0
  config.dataset_configs.augmentation_params.do_color_augment = False
  config.dataset_configs.augmentation_params.prob_color_augment = 0.8
  config.dataset_configs.augmentation_params.prob_color_drop = 0.1

  # This does Mixup in the train loop. This is fast. But make sure that device
  # batch size is more than 1.
  config.mixup = ml_collections.ConfigDict()
  config.mixup.alpha = 0.1

  config.dataset_configs.augmentation_params.do_rand_augment = True
  config.dataset_configs.augmentation_params.rand_augment_num_layers = 2
  config.dataset_configs.augmentation_params.rand_augment_magnitude = 15

  config.dataset_configs.prefetch_to_device = 2

  # Model.
  config.model_name = 'vivit_multihead_classification'
  config.model = ml_collections.ConfigDict()
  config.model.hidden_size = 1024
  config.model.patches = ml_collections.ConfigDict()
  config.model.patches.size = [16, 16]

  config.model.attention_config = ml_collections.ConfigDict()
  config.model.attention_config.type = 'factorized_encoder'

  config.model.spatial_transformer = ml_collections.ConfigDict()
  config.model.spatial_transformer.num_heads = 16
  config.model.spatial_transformer.mlp_dim = 4096
  config.model.spatial_transformer.num_layers = 24

  config.model.temporal_transformer = ml_collections.ConfigDict()
  config.model.temporal_transformer.num_heads = 16
  config.model.temporal_transformer.mlp_dim = 4096
  config.model.temporal_transformer.num_layers = 4

  config.model.representation_size = None
  config.model.classifier = 'token'
  config.model.attention_dropout_rate = 0.
  config.model.dropout_rate = 0.
  config.model.stochastic_droplayer_rate = 0.2
  config.model_dtype_str = 'float32'

  config.model.temporal_encoding_config = ml_collections.ConfigDict()
  config.model.temporal_encoding_config.method = '3d_conv'
  config.model.patches.size = [16, 16, 2]
  config.model.temporal_encoding_config.kernel_init_method = 'central_frame_initializer'

  # Training.
  config.trainer_name = 'vivit_trainer'
  config.optimizer = 'momentum'
  config.optimizer_configs = ml_collections.ConfigDict()
  config.l2_decay_factor = 0
  config.max_grad_norm = 1
  config.label_smoothing = 0.2
  config.num_training_epochs = 50
  config.batch_size = 64
  config.rng_seed = 0

  # Initialisation.
  config.init_from = ml_collections.ConfigDict()
  Use a pretrained Kinetics checkpoint, or ImageNet checkpoint
  config.init_from.checkpoint_path = 'path_to_checkpoint'
  config.init_from.checkpoint_format = 'scenic'
  config.init_from.model_config = ml_collections.ConfigDict()
  config.init_from.model_config.model = ml_collections.ConfigDict()
  config.init_from.model_config.model.classifier = 'token'  # Specify if this is 'token' or 'gap'.  pylint: disable=line-too-long
  config.init_from.checkpoint_path = None
  config.init_from.restore_positional_embedding = True
  config.init_from.restore_input_embedding = True
  config.init_from.positional_embed_size_change = 'tile'

  # Learning rate.
  steps_per_epoch = EPIC_TRAIN_SIZE // config.batch_size
  total_steps = config.num_training_epochs * steps_per_epoch
  config.lr_configs = ml_collections.ConfigDict()
  config.lr_configs.learning_rate_schedule = 'compound'
  config.lr_configs.factors = 'constant * cosine_decay * linear_warmup'
  config.lr_configs.warmup_steps = int(2.5 * steps_per_epoch)
  config.lr_configs.steps_per_cycle = total_steps
  config.lr_configs.base_learning_rate = 0.5

  # Logging.
  config.write_summary = True
  config.checkpoint = True  # Do checkpointing.
  config.debug_train = False  # Debug mode during training.
  config.debug_eval = False  # Debug mode during eval.
  config.checkpoint_steps = 500  # Checkpoint more frequently than a val epoch.
  config.log_summary_steps = 100
  return config





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/configs/epic_kitchens/__init__.py ===





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/configs/kinetics600/vivit_large_factorised_encoder.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

r"""ViViT Factorised Encoder model.

"""

import ml_collections

# The size of the Kinetics dataset changes as videos are removed from YouTube.
# Set this appropriately.
KINETICS_600_TRAIN_SIZE = 363213
KINETICS_600_VAL_SIZE = 27676
KINETICS_600_TEST_SIZE = 55377


def get_config():
  """Returns the base experiment configuration."""
  config = ml_collections.ConfigDict()
  config.experiment_name = 'vivit_large_factorised_encoder'

  # Dataset.
  config.dataset_name = 'video_tfrecord_dataset'
  config.dataset_configs = ml_collections.ConfigDict()
  config.data_dtype_str = 'float32'
  config.datset_configs = ml_collections.ConfigDict()
  config.dataset_configs.base_dir = (
      '/path/to/dataset')
  config.dataset_configs.tables = {
      'train': 'train.tfrecord@1024',
      'validation': 'validation.tfrecord@1024',
      'test': 'test.tfrecord@1024'
  }
  config.dataset_configs.examples_per_subset = {
      'train': KINETICS_600_TRAIN_SIZE,
      'validation': KINETICS_600_VAL_SIZE,
      'test': KINETICS_600_TEST_SIZE
  }
  config.dataset_configs.num_classes = 600
  config.data_dtype_str = 'float32'

  # This is going to sample 32 frames, sampled at a stride of 2 from the video.
  # Kinetics videos has 250 frames.
  config.dataset_configs.num_frames = 32
  config.dataset_configs.stride = 2
  config.dataset_configs.min_resize = 256
  config.dataset_configs.crop_size = 224
  config.dataset_configs.one_hot_labels = True
  config.dataset_configs.zero_centering = True

  # Multicrop evaluation settings:
  config.dataset_configs.do_multicrop_test = True  # Do during training.
  config.dataset_configs.log_test_epochs = 5
  # The effective batch size per host when testing is
  # num_test_clips * test_batch_size.
  config.dataset_configs.num_test_clips = 4
  config.dataset_configs.test_batch_size = 8  # Must equal num_local_devices.
  # To take three spatial crops when testing.
  config.dataset_configs.do_three_spatial_crops = True
  config.multicrop_clips_per_device = 2

  # Data augmentation.
  config.dataset_configs.augmentation_params = ml_collections.ConfigDict()
  config.dataset_configs.augmentation_params.do_jitter_scale = True
  config.dataset_configs.augmentation_params.scale_min_factor = 0.9
  config.dataset_configs.augmentation_params.scale_max_factor = 1.33
  config.dataset_configs.augmentation_params.prob_scale_jitter = 1.0
  config.dataset_configs.augmentation_params.do_color_augment = True
  config.dataset_configs.augmentation_params.prob_color_augment = 0.8
  config.dataset_configs.augmentation_params.prob_color_drop = 0.1
  config.dataset_configs.prefetch_to_device = 2

  # Model.
  config.model_name = 'vivit_classification'
  config.model = ml_collections.ConfigDict()
  config.model.hidden_size = 1024

  config.model.attention_config = ml_collections.ConfigDict()
  config.model.attention_config.type = 'factorized_encoder'
  config.model.spatial_transformer = ml_collections.ConfigDict()
  config.model.spatial_transformer.num_heads = 16
  config.model.spatial_transformer.mlp_dim = 4096
  config.model.spatial_transformer.num_layers = 24
  config.model.temporal_transformer = ml_collections.ConfigDict()
  config.model.temporal_transformer.num_heads = 16
  config.model.temporal_transformer.mlp_dim = 4096
  config.model.temporal_transformer.num_layers = 24
  config.model.representation_size = None
  config.model.classifier = 'token'
  config.model.attention_dropout_rate = 0.
  config.model.dropout_rate = 0.
  config.model_dtype_str = 'float32'

  config.model.temporal_encoding_config = ml_collections.ConfigDict()
  config.model.temporal_encoding_config.method = '3d_conv'

  config.model.patches = ml_collections.ConfigDict()
  config.model.patches.size = [16, 16, 2]

  config.model.temporal_encoding_config.kernel_init_method = 'central_frame_initializer'

  # Training.
  config.trainer_name = 'vivit_trainer'
  config.optimizer = 'momentum'
  config.optimizer_configs = ml_collections.ConfigDict()
  config.l2_decay_factor = 0
  config.max_grad_norm = 1
  config.label_smoothing = None
  config.num_training_epochs = 30
  config.batch_size = 64
  config.rng_seed = 0

  # Use ImageNet-21k-initialized model.
  config.init_from = ml_collections.ConfigDict()
  config.init_from.model_config = None
  # Download pretrained ImageNet checkpoints from here:
  # https://github.com/google-research/scenic/tree/main/scenic/projects/baselines (checkpoint_format = 'scenic')  pylint: disable=line-too-long
  # https://github.com/google-research/vision_transformer (checkpoint_format = 'big_vision')  pylint: disable=line-too-long
  config.init_from.checkpoint_path = 'path_to_checkpoint_of_vit_b_16'
  config.init_from.checkpoint_format = 'scenic'
  config.init_from.model_config = ml_collections.ConfigDict()
  config.init_from.model_config.model = ml_collections.ConfigDict()
  config.init_from.model_config.model.classifier = 'token'  # Specify if this is 'token' or 'gap'.  pylint: disable=line-too-long
  config.init_from.restore_positional_embedding = True
  config.init_from.restore_input_embedding = True
  config.init_from.positional_embed_size_change = 'tile'

  # Learning rate.
  steps_per_epoch = KINETICS_600_TRAIN_SIZE // config.batch_size
  total_steps = config.num_training_epochs * steps_per_epoch
  config.lr_configs = ml_collections.ConfigDict()
  config.lr_configs.learning_rate_schedule = 'compound'
  config.lr_configs.factors = 'constant * cosine_decay * linear_warmup'
  config.lr_configs.warmup_steps = 2.5 * steps_per_epoch
  config.lr_configs.steps_per_cycle = total_steps
  config.lr_configs.base_learning_rate = 5e-2

  # Logging.
  config.write_summary = True
  config.checkpoint = True  # Do checkpointing.
  config.debug_train = False  # Debug mode during training.
  config.debug_eval = False  # Debug mode during eval.
  config.checkpoint_steps = 500  # Checkpoint more frequently than a val epoch.
  config.log_summary_steps = 100
  return config





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/configs/kinetics600/__init__.py ===





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/configs/kinetics400/vivit_base_factorised_encoder.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

r"""ViViT Base Factorised Encoder model.

"""

import ml_collections

# The size of the Kinetics dataset changes as videos are removed from YouTube.
# Set this appropriately.
KINETICS_400_TRAIN_SIZE = 214834
KINETICS_400_VAL_SIZE = 17637
KINETICS_400_TEST_SIZE = 34579


def get_config():
  """Returns the base experiment configuration."""
  config = ml_collections.ConfigDict()
  config.experiment_name = 'vivit_kinetics400_classification'

  # Dataset.
  config.dataset_name = 'video_tfrecord_dataset'
  config.dataset_configs = ml_collections.ConfigDict()
  config.data_dtype_str = 'float32'
  config.datset_configs = ml_collections.ConfigDict()
  config.dataset_configs.base_dir = (
      '/path/to/dataset')
  config.dataset_configs.tables = {
      'train': 'train.tfrecord@1024',
      'validation': 'validation.tfrecord@1024',
      'test': 'test.tfrecord@1024'
  }
  config.dataset_configs.examples_per_subset = {
      'train': KINETICS_400_TRAIN_SIZE,
      'validation': KINETICS_400_VAL_SIZE,
      'test': KINETICS_400_TEST_SIZE
  }
  config.dataset_configs.num_classes = 400
  config.data_dtype_str = 'float32'

  # This is going to sample 32 frames, sampled at a stride of 2 from the video.
  # Kinetics videos has 250 frames.
  config.dataset_configs.num_frames = 32
  config.dataset_configs.stride = 2
  config.dataset_configs.min_resize = 256
  config.dataset_configs.crop_size = 224
  config.dataset_configs.one_hot_labels = True
  config.dataset_configs.zero_centering = True

  # Multicrop evaluation settings:
  config.dataset_configs.do_multicrop_test = True  # Do during training.
  config.dataset_configs.log_test_epochs = 5
  # The effective batch size per host when testing is
  # num_test_clips * test_batch_size.
  config.dataset_configs.num_test_clips = 4
  config.dataset_configs.test_batch_size = 8  # Must equal num_local_devices.
  # To take three spatial crops when testing.
  config.dataset_configs.do_three_spatial_crops = True
  config.multicrop_clips_per_device = 2

  # Data augmentation.
  config.dataset_configs.augmentation_params = ml_collections.ConfigDict()
  config.dataset_configs.augmentation_params.do_jitter_scale = True
  config.dataset_configs.augmentation_params.scale_min_factor = 0.9
  config.dataset_configs.augmentation_params.scale_max_factor = 1.33
  config.dataset_configs.augmentation_params.prob_scale_jitter = 1.0
  config.dataset_configs.augmentation_params.do_color_augment = True
  config.dataset_configs.augmentation_params.prob_color_augment = 0.8
  config.dataset_configs.augmentation_params.prob_color_drop = 0.1
  config.dataset_configs.prefetch_to_device = 2

  # Model.
  config.model_name = 'vivit_classification'
  config.model = ml_collections.ConfigDict()
  config.model.hidden_size = 768

  config.model.attention_config = ml_collections.ConfigDict()
  config.model.attention_config.type = 'factorized_encoder'

  config.model.patches = ml_collections.ConfigDict()
  config.model.spatial_transformer = ml_collections.ConfigDict()
  config.model.spatial_transformer.num_heads = 12
  config.model.spatial_transformer.mlp_dim = 3072
  config.model.spatial_transformer.num_layers = 12
  config.model.temporal_transformer = ml_collections.ConfigDict()
  config.model.temporal_transformer.num_heads = 12
  config.model.temporal_transformer.mlp_dim = 3072
  config.model.temporal_transformer.num_layers = 4
  config.model.representation_size = None
  config.model.classifier = 'token'
  config.model.attention_dropout_rate = 0.
  config.model.dropout_rate = 0.
  config.model_dtype_str = 'float32'
  config.model.temporal_encoding_config = ml_collections.ConfigDict()
  config.model.temporal_encoding_config.method = '3d_conv'
  config.model.patches.size = (16, 16, 2)

  config.model.temporal_encoding_config.kernel_init_method = 'central_frame_initializer'
  # Applies when temporal_encoding_config.method='temporal_sampling'
  config.model.temporal_encoding_config.n_sampled_frames = 16  # Unused here.

  # Training.
  config.trainer_name = 'vivit_trainer'
  config.optimizer = 'momentum'
  config.optimizer_configs = ml_collections.ConfigDict()
  config.l2_decay_factor = 0
  config.max_grad_norm = 1
  config.label_smoothing = None
  config.num_training_epochs = 30
  config.batch_size = 64
  config.rng_seed = 0

  # Use ImageNet-21k-initialized model.
  config.init_from = ml_collections.ConfigDict()
  config.init_from.model_config = None
  # Download pretrained ImageNet checkpoints from here:
  # https://github.com/google-research/scenic/tree/main/scenic/projects/baselines (checkpoint_format = 'scenic')  pylint: disable=line-too-long
  # https://github.com/google-research/vision_transformer (checkpoint_format = 'big_vision')  pylint: disable=line-too-long
  config.init_from.checkpoint_path = 'path_to_checkpoint_of_vit_b_16'
  config.init_from.checkpoint_format = 'scenic'
  config.init_from.model_config = ml_collections.ConfigDict()
  config.init_from.model_config.model = ml_collections.ConfigDict()
  config.init_from.model_config.model.classifier = 'token'  # Specify if this is 'token' or 'gap'.  pylint: disable=line-too-long
  config.init_from.restore_positional_embedding = True
  config.init_from.restore_input_embedding = True
  config.init_from.positional_embed_size_change = 'tile'

  # Learning rate.
  steps_per_epoch = KINETICS_400_TRAIN_SIZE // config.batch_size
  total_steps = config.num_training_epochs * steps_per_epoch
  config.lr_configs = ml_collections.ConfigDict()
  config.lr_configs.learning_rate_schedule = 'compound'
  config.lr_configs.factors = 'constant * cosine_decay * linear_warmup'
  config.lr_configs.warmup_steps = int(2.5 * steps_per_epoch)
  config.lr_configs.steps_per_cycle = total_steps
  config.lr_configs.base_learning_rate = 5e-2

  # Logging.
  config.write_summary = True
  config.checkpoint = True  # Do checkpointing.
  config.debug_train = False  # Debug mode during training.
  config.debug_eval = False  # Debug mode during eval.
  config.checkpoint_steps = 1000  # Checkpoint more frequently than a val epoch.
  return config





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/configs/kinetics400/vivit_base_k400.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

r"""ViViT-Base Unfactorised.

"""


from absl import logging
import ml_collections

# The size of the Kinetics dataset changes as videos are removed from YouTube.
# Set this appropriately.
KINETICS_400_TRAIN_SIZE = 214834
KINETICS_400_VAL_SIZE = 17637
KINETICS_400_TEST_SIZE = 34579


def get_config():
  """Returns the base experiment configuration."""
  config = ml_collections.ConfigDict()
  config.experiment_name = 'vivit_base_kinetics400_im21k'

  # Dataset.
  config.dataset_name = 'video_tfrecord_dataset'
  config.dataset_configs = ml_collections.ConfigDict()
  config.data_dtype_str = 'float32'
  config.datset_configs = ml_collections.ConfigDict()
  config.dataset_configs.base_dir = (
      '/path/to/dataset')
  config.dataset_configs.tables = {
      'train': 'train.tfrecord@1024',
      'validation': 'validation.tfrecord@1024',
      'test': 'test.tfrecord@1024'
  }
  config.dataset_configs.examples_per_subset = {
      'train': KINETICS_400_TRAIN_SIZE,
      'validation': KINETICS_400_VAL_SIZE,
      'test': KINETICS_400_TEST_SIZE
  }
  config.dataset_configs.num_classes = 400
  config.data_dtype_str = 'float32'

  # This is going to sample 32 frames, sampled at a stride of 2 from the video.
  # Kinetics videos has 250 frames.
  config.dataset_configs.num_frames = 32
  config.dataset_configs.stride = 2
  config.dataset_configs.min_resize = 256
  config.dataset_configs.crop_size = 224
  config.dataset_configs.one_hot_labels = True
  config.dataset_configs.zero_centering = True

  # Multicrop evaluation settings:
  config.dataset_configs.do_multicrop_test = True  # Do during training.
  config.dataset_configs.log_test_epochs = 5
  # The effective batch size per host when testing is
  # num_test_clips * test_batch_size.
  config.dataset_configs.num_test_clips = 4
  config.dataset_configs.test_batch_size = 8  # Must equal num_local_devices.
  # To take three spatial crops when testing.
  config.dataset_configs.do_three_spatial_crops = True
  config.multicrop_clips_per_device = 2

  # Data augmentation.
  config.dataset_configs.augmentation_params = ml_collections.ConfigDict()
  config.dataset_configs.augmentation_params.do_jitter_scale = True
  config.dataset_configs.augmentation_params.scale_min_factor = 0.9
  config.dataset_configs.augmentation_params.scale_max_factor = 1.33
  config.dataset_configs.augmentation_params.prob_scale_jitter = 1.0
  config.dataset_configs.augmentation_params.do_color_augment = True
  config.dataset_configs.augmentation_params.prob_color_augment = 0.8
  config.dataset_configs.augmentation_params.prob_color_drop = 0.1

  config.dataset_configs.prefetch_to_device = 2

  # Model: ViViT-B
  config.model_name = 'vivit_classification'
  config.model = ml_collections.ConfigDict()
  config.model.attention_config = ml_collections.ConfigDict()
  config.model.attention_config.type = 'spacetime'

  config.model.hidden_size = 768
  config.model.patches = ml_collections.ConfigDict()
  config.model.num_heads = 12
  config.model.mlp_dim = 3072
  config.model.num_layers = 12
  config.model.representation_size = None
  config.model.classifier = 'token'
  config.model.attention_dropout_rate = 0.
  config.model.dropout_rate = 0.
  config.model_dtype_str = 'float32'

  config.model.temporal_encoding_config = ml_collections.ConfigDict()
  config.model.temporal_encoding_config.method = '3d_conv'
  config.model.patches.size = (16, 16, 2)
  config.model.temporal_encoding_config.kernel_init_method = 'central_frame_initializer'

  # Training.
  config.trainer_name = 'vivit_trainer'
  config.optimizer = 'momentum'
  config.optimizer_configs = ml_collections.ConfigDict()
  config.l2_decay_factor = 0
  config.max_grad_norm = 1
  config.label_smoothing = None
  config.num_training_epochs = 30
  config.batch_size = 64
  config.rng_seed = 0

  # Use ImageNet-21k-initialized model.
  config.init_from = ml_collections.ConfigDict()
  config.init_from.model_config = None
  # Download pretrained ImageNet checkpoints from here:
  # https://github.com/google-research/scenic/tree/main/scenic/projects/baselines (checkpoint_format = 'scenic')  pylint: disable=line-too-long
  # https://github.com/google-research/vision_transformer (checkpoint_format = 'big_vision')  pylint: disable=line-too-long
  config.init_from.checkpoint_path = 'path_to_checkpoint_of_vit_b_16'
  config.init_from.checkpoint_format = 'scenic'
  config.init_from.model_config = ml_collections.ConfigDict()
  config.init_from.model_config.model = ml_collections.ConfigDict()
  config.init_from.model_config.model.classifier = 'token'  # Specify if this is 'token' or 'gap'.  pylint: disable=line-too-long
  config.init_from.restore_positional_embedding = True
  config.init_from.restore_input_embedding = True
  config.init_from.positional_embed_size_change = 'tile'

  # Learning rate.
  steps_per_epoch = KINETICS_400_TRAIN_SIZE // config.batch_size
  total_steps = config.num_training_epochs * steps_per_epoch
  config.lr_configs = ml_collections.ConfigDict()
  config.lr_configs.learning_rate_schedule = 'compound'
  config.lr_configs.factors = 'constant * cosine_decay * linear_warmup'
  config.lr_configs.warmup_steps = int(2.5 * steps_per_epoch)
  config.lr_configs.steps_per_cycle = total_steps
  config.lr_configs.base_learning_rate = 1e-1

  # Logging.
  config.write_summary = True
  config.checkpoint = True  # Do checkpointing.
  config.debug_train = False  # Debug mode during training.
  config.debug_eval = False  # Debug mode during eval.
  return config





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/configs/kinetics400/vivit_large_factorised_encoder.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

r"""ViVIT Large Factorised Encoder.

"""

import ml_collections

# The size of the Kinetics dataset changes as videos are removed from YouTube.
# Set this appropriately.
KINETICS_400_TRAIN_SIZE = 214834
KINETICS_400_VAL_SIZE = 17637
KINETICS_400_TEST_SIZE = 34579


def get_config():
  """Returns the base experiment configuration."""
  config = ml_collections.ConfigDict()
  config.experiment_name = 'vivit_large_factorised_encoder_num_frames_im21k'

  # Dataset.
  config.dataset_name = 'video_tfrecord_dataset'
  config.dataset_configs = ml_collections.ConfigDict()
  config.data_dtype_str = 'float32'
  config.datset_configs = ml_collections.ConfigDict()
  config.dataset_configs.base_dir = (
      '/path/to/dataset')
  config.dataset_configs.tables = {
      'train': 'train.tfrecord@1024',
      'validation': 'validation.tfrecord@1024',
      'test': 'test.tfrecord@1024'
  }
  config.dataset_configs.examples_per_subset = {
      'train': KINETICS_400_TRAIN_SIZE,
      'validation': KINETICS_400_VAL_SIZE,
      'test': KINETICS_400_TEST_SIZE
  }
  config.dataset_configs.num_classes = 400
  config.data_dtype_str = 'float32'

  # This is going to sample 32 frames, sampled at a stride of 2 from the video.
  # Kinetics videos has 250 frames.
  config.dataset_configs.num_frames = 32
  config.dataset_configs.stride = 2
  config.dataset_configs.min_resize = 256
  config.dataset_configs.crop_size = 224
  config.dataset_configs.one_hot_labels = True
  config.dataset_configs.zero_centering = True

  # Multicrop evaluation settings:
  config.dataset_configs.do_multicrop_test = True  # Do during training.
  config.dataset_configs.log_test_epochs = 5
  # The effective batch size per host when testing is
  # num_test_clips * test_batch_size.
  config.dataset_configs.num_test_clips = 4
  config.dataset_configs.test_batch_size = 8  # Must equal num_local_devices.
  # To take three spatial crops when testing.
  config.dataset_configs.do_three_spatial_crops = True
  config.multicrop_clips_per_device = 2

  # Data augmentation.
  config.dataset_configs.augmentation_params = ml_collections.ConfigDict()
  config.dataset_configs.augmentation_params.do_jitter_scale = True
  config.dataset_configs.augmentation_params.scale_min_factor = 0.9
  config.dataset_configs.augmentation_params.scale_max_factor = 1.33
  config.dataset_configs.augmentation_params.prob_scale_jitter = 1.0
  config.dataset_configs.augmentation_params.do_color_augment = True
  config.dataset_configs.augmentation_params.prob_color_augment = 0.8
  config.dataset_configs.augmentation_params.prob_color_drop = 0.1
  config.dataset_configs.prefetch_to_device = 2

  # Model.
  config.model_name = 'vivit_classification'
  config.model = ml_collections.ConfigDict()
  config.model.hidden_size = 1024

  config.model.attention_config = ml_collections.ConfigDict()
  config.model.attention_config.type = 'factorized_encoder'
  config.model.spatial_transformer = ml_collections.ConfigDict()
  config.model.spatial_transformer.num_heads = 16
  config.model.spatial_transformer.mlp_dim = 4096
  config.model.spatial_transformer.num_layers = 24
  config.model.temporal_transformer = ml_collections.ConfigDict()
  config.model.temporal_transformer.num_heads = 16
  config.model.temporal_transformer.mlp_dim = 4096
  config.model.temporal_transformer.num_layers = 24
  config.model.representation_size = None
  config.model.classifier = 'token'
  config.model.attention_dropout_rate = 0.
  config.model.dropout_rate = 0.
  config.model_dtype_str = 'float32'

  config.model.temporal_encoding_config = ml_collections.ConfigDict()
  config.model.temporal_encoding_config.method = '3d_conv'

  config.model.patches = ml_collections.ConfigDict()
  config.model.patches.size = (16, 16, 2)

  config.model.temporal_encoding_config.kernel_init_method = 'central_frame_initializer'

  # Training.
  config.trainer_name = 'vivit_trainer'
  config.optimizer = 'momentum'
  config.optimizer_configs = ml_collections.ConfigDict()
  config.l2_decay_factor = 0
  config.max_grad_norm = 1
  config.label_smoothing = None
  config.num_training_epochs = 30
  config.batch_size = 64
  config.rng_seed = 0

  # Use ImageNet-21k-initialized model.
  config.init_from = ml_collections.ConfigDict()
  config.init_from.model_config = None
  # Download pretrained ImageNet checkpoints from here:
  # https://github.com/google-research/scenic/tree/main/scenic/projects/baselines (checkpoint_format = 'scenic')  pylint: disable=line-too-long
  # https://github.com/google-research/vision_transformer (checkpoint_format = 'big_vision')  pylint: disable=line-too-long
  config.init_from.checkpoint_path = 'path_to_checkpoint_of_vit_b_16'
  config.init_from.checkpoint_format = 'scenic'
  config.init_from.model_config = ml_collections.ConfigDict()
  config.init_from.model_config.model = ml_collections.ConfigDict()
  config.init_from.model_config.model.classifier = 'token'  # Specify if this is 'token' or 'gap'.  pylint: disable=line-too-long
  config.init_from.restore_positional_embedding = True
  config.init_from.restore_input_embedding = True
  config.init_from.positional_embed_size_change = 'tile'

  # Learning rate.
  steps_per_epoch = KINETICS_400_TRAIN_SIZE // config.batch_size
  total_steps = config.num_training_epochs * steps_per_epoch
  config.lr_configs = ml_collections.ConfigDict()
  config.lr_configs.learning_rate_schedule = 'compound'
  config.lr_configs.factors = 'constant * cosine_decay * linear_warmup'
  config.lr_configs.warmup_steps = int(2.5 * steps_per_epoch)
  config.lr_configs.steps_per_cycle = total_steps
  config.lr_configs.base_learning_rate = 1e-1

  # Logging.
  config.write_summary = True
  config.checkpoint = True  # Do checkpointing.
  config.debug_train = False  # Debug mode during training.
  config.debug_eval = False  # Debug mode during eval.
  config.checkpoint_steps = 500  # Checkpoint more frequently than a val epoch.
  return config





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/configs/kinetics400/__init__.py ===





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/data/file_utils.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Utility functions for working with sharded files.

A sharded file is a single conceptual file that is broken into a collection
of files to make parallelization easier.  A sharded file spec is like a
filename for a sharded file; the file spec "/some/path/prefix@200.txt"
says that the sharded file consists of 200 actual files that have names like
"/some/path/prefix-00000-of-00200.txt", "/some/path/prefix-00001-of-00200.txt",
etc.  This module contains functions for parsing, generating and detecting
sharded file specs.
"""

import math
import re
from typing import Iterator, Tuple

SHARD_SPEC_PATTERN = re.compile(R'((.*)\@(\d*[1-9]\d*)(?:\.(.+))?)')


class ShardError(Exception):
  """An I/O error."""


def parse_sharded_file_spec(spec: str) -> Tuple[str, int, str]:
  """Parse a sharded file specification.

  Args:
    spec: The sharded file specification. A sharded file spec is one like
      'gs://some/file@200.txt'. Here, '@200' specifies the number of shards.

  Returns:
    basename: The basename for the files.
    num_shards: The number of shards.
    suffix: The suffix if there is one, or '' if not.
  Raises:
    ShardError: If the spec is not a valid sharded specification.
  """
  m = SHARD_SPEC_PATTERN.match(spec)
  if not m:
    raise ShardError(('The file specification {0} is not a sharded file '
                      'specification because it did not match the regex '
                      '{1}').format(spec, SHARD_SPEC_PATTERN.pattern))

  # If there's a non-empty suffix, we need to prepend '.' so we get files like
  # foo@20.ext instead of foo@ext
  suffix = '.' + m.group(4) if m.group(4) else ''

  return m.group(2), int(m.group(3)), suffix


def _shard_width(num_shards: int) -> int:
  """Return the width of the shard matcher based on the number of shards."""
  return max(5, int(math.floor(math.log10(num_shards)) + 1))


def generate_sharded_filenames(spec: str) -> Iterator[str]:
  """Generator for the list of filenames corresponding to the sharding path.

  Args:
    spec: Represents a filename with a sharding specification.
      e.g., 'gs://some/file@200.txt' represents a file sharded 200 ways.

  Yields:
    Each filename in the sharding path.

  Raises:
    ShardError: If spec is not a valid sharded file specification.
  """
  basename, num_shards, suffix = parse_sharded_file_spec(spec)
  width = _shard_width(num_shards)
  format_str = '{{0}}-{{1:0{0}}}-of-{{2:0{0}}}{{3}}'.format(width)
  for i in range(num_shards):
    yield format_str.format(basename, i, num_shards, suffix)


def is_sharded_file_spec(spec: str) -> bool:
  """Returns True if spec is a sharded file specification."""
  m = SHARD_SPEC_PATTERN.match(spec)
  return m is not None



=== File: /home/ndelafuente/scenic/scenic/projects/vivit/data/data.md ===


Datasets
==

The training pipeline uses the [DeepMind Video Reader (DMVR)](https://github.com/deepmind/dmvr)
library for pre-processing and data-augmentation.
Futhermore, we assume that datasets are stored in in [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) files.

To pre-process a dataset into the required format, please follow the
instructions from DMVR [here](https://github.com/deepmind/dmvr/tree/master/examples).

Once a dataset has been pre-processed, it can easily be used for training by
adding the following snippet to the configuration file:

```python
dataset_configs = ml_collections.ConfigDict()

dataset_configs.base_dir = '/path/to/dataset_root/'
dataset_configs.tables = {
    'train': 'relative_path_to_train_set',
    'validation': 'relative_path_to_validation_set',
    'test': 'relative_path_to_test_set'
}
dataset_configs.examples_per_subset = {
    'train': NUM_TRAIN_EXAMPLES,
    'validation': NUM_VAL_EXAMPLES,
    'test': NUM_TEST_EXAMPLES
}
dataset_configs.num_classes = NUM_CLASSES
```



=== File: /home/ndelafuente/scenic/scenic/projects/vivit/data/video_tfrecord_dataset.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Data-loader to read from SSTables using the MediaSequence format."""

import functools
import os
from typing import Dict, Iterator, List, Optional, Text, Tuple, Union

from absl import logging
from dmvr import builders
from dmvr import modalities
from dmvr import video_dataset
from flax import jax_utils
import jax
import jax.numpy as jnp
import ml_collections
import numpy as np
from scenic.dataset_lib import dataset_utils
from scenic.dataset_lib import datasets
from scenic.dataset_lib import video_ops
from scenic.projects.vivit.data import file_utils
import tensorflow as tf

# Aliases for custom types:
Batch = Dict[str, jnp.ndarray]
Rng = Union[jnp.ndarray, Dict[str, jnp.ndarray]]


def get_sharded_files(
    data_path: str,
    fraction_data: float = 1.0,
    num_groups: Optional[int] = None,
    group_index: Optional[int] = None) -> List[str]:
  """Returns a list of shards, which may be postprocessed.

  Args:
    data_path: Path to the data, either sharded or a single file.
    fraction_data: Fraction of the data to be consumed. Only that fraction of
      the shards is returned.
    num_groups: Number of groups to split the data. All the shards will be split
      in `num_groups` groups (of approximately same number of files) and the
      given `group_index` group only will be returned. This is useful when
      distributing the data over multiple hosts, which will make sure that the
      same shard is not processed in two different hosts. If `num_groups` is
      provided `group_index` must be provided as well.
    group_index: Index of the group of data being returned. See `num_groups`.

  Returns:
    A list of shard filenames.

  Raises:
    ValueError: If `fraction_data` is not between 0 and 1.
    ValueError: If `num_groups` requested is not consistent with the number of
      shards available.
    ValueError: If `group_index` >= `num_groups`
    ValueError: If only one of `num_groups` and `group_index` is given.
  """
  if fraction_data <= 0 or fraction_data > 1:
    raise ValueError(
        f'The fraction of data must be in (0, 1] but is {fraction_data}.')

  if file_utils.is_sharded_file_spec(data_path):
    shards = list(file_utils.generate_sharded_filenames(data_path))
  else:
    shards = [data_path]

  num_used_shards = int(np.ceil(fraction_data * len(shards)))
  shards = shards[:num_used_shards]

  if num_groups is None and group_index is None:
    return shards
  if num_groups is None or group_index is None:
    raise ValueError('Both `num_groups` and `group_index` should be specified.')
  if group_index >= num_groups:
    raise ValueError(
        f'Cannot request index {group_index} of {num_groups} groups')
  if num_groups > num_used_shards:
    raise ValueError(
        f'After applying `fraction_data={fraction_data}` we have '
        f'{num_used_shards} data shards, which cannot be split into '
        f'{num_groups} groups.')

  split_shard_ids = np.array_split(np.arange(num_used_shards), num_groups)
  begin_loc = split_shard_ids[group_index][0]
  end_loc = split_shard_ids[group_index][-1] + 1
  shards = shards[begin_loc:end_loc]
  return shards


class TFRecordDatasetFactory(video_dataset.BaseVideoDatasetFactory):
  """Reader for TFRecords using the MediaSequence format.

  Attributes:
    num_classes: int. The number of classes in the dataset.
    base_dir: str. The base directory from which the TFRecords are read.
    subset: str. The subset of the dataset. In Scenic, the subsets are
      "train", "validation" and "test".
  """

  def __init__(
      self,
      base_dir: str,
      tables: Dict[str, Union[str, List[str]]],
      examples_per_subset: Dict[str, int],
      num_classes: int,
      subset: str = 'train',
      fraction_data: float = 1.0,
      num_groups: Optional[int] = None,
      group_index: Optional[int] = None):
    """Initializes the instance of TFRecordDatasetFactory.

    Initializes a data-loader using DeepMind Video Reader (DMVR) pre-processing
    (https://github.com/deepmind/dmvr).
    TFRecords are assumed to consist of tf.SequenceExample protocol buffers in
    the MediaSequence
    (https://github.com/google/mediapipe/tree/master/mediapipe/util/sequence)
    format.

    Args:
      base_dir: The base directory of the TFRecords.
      tables: A dictionary mapping the subset name (train, val or test) to the
        relative path of the TFRecord containing them. Follows DMVR convention.
        The values of the dictionary can either be a string or a list. If it is
        a string, it specifies all the shards in the TFRecord.
        Example - "/path/to/tfrecord@10".
        If passing a list, each entry is a shard of the TFRecord.
        Example - "[/path/to/tfrecord_shard_1_of_10, ...,
                    /path/to/tfrecord_shard_10_of_10]."
        The latter scenario is useful for debugging.
      examples_per_subset:  A dictionary mapping the subset name (train, val or
        test) to the number of examples in the dataset for that subset.
      num_classes: The number of classes in the dataset.
      subset: The subset of the dataset to load. Must be a key of "tables"
      fraction_data: The fraction of the data to load. If less than 1.0, this
        fraction of the total TFRecord shards are read.
      num_groups: If specified will reshard the data according to `num_groups`.
        A `group_index` should be specified if using `num_groups`.
      group_index: Index of the shard to return after resharding. `num_groups`
        should be specified if using `group_index`. This is useful in
        distributed setting where one wants to ensure that different data is
        read by different workers.

    Raises:
      ValueError: If subset is not a key of tables or examples_per_subset
    """
    if (subset not in tables) or (subset not in examples_per_subset):
      raise ValueError(f'Invalid subset {subset!r}. '
                       f'The available subsets are: {set(tables)!r}')

    self.num_classes = num_classes
    self.base_dir = base_dir
    self.subset = subset
    self.num_examples = examples_per_subset[subset]

    data_relative_path = tables[subset]
    if isinstance(data_relative_path, list):
      shards = [os.path.join(self.base_dir, x) for x in data_relative_path]
    else:
      data_path = os.path.join(self.base_dir, data_relative_path)
      shards = get_sharded_files(data_path=data_path,
                                 fraction_data=fraction_data,
                                 num_groups=num_groups,
                                 group_index=group_index)

    super().__init__(shards=shards)

  def _build(self,
             is_training: bool = True,
             # Video related parameters.
             num_frames: int = 32,
             stride: int = 1,
             num_test_clips: int = 1,
             min_resize: int = 256,
             crop_size: int = 224,
             zero_centering_image: bool = False,
             train_frame_sampling_mode: str = 'random',
             # Label related parameters.
             one_hot_label: bool = True,
             get_label_str: bool = False,
             label_offset: int = 0):
    """Adds DMVR pre-processors to the dataset.

    Args:
      is_training: whether or not in training mode.
      num_frames: number of frames per subclip.
      stride: temporal stride to sample frames.
      num_test_clips: number of test clip (1 by default). If more than one,
        this will sample multiple linearly spaced clips within each video at
        test time. If 1, then a single clip in the middle of the video is
        sampled.
      min_resize: frames are resized so that min width/height is min_resize.
      crop_size: final size of the frame after cropping the resized frames.
      zero_centering_image: whether to have image values in the interval [-1, 1]
        or [0, 1].
      train_frame_sampling_mode: Method of sampling frames during training.
        Options are one of {random, random_sample_with_centre, centre}
      one_hot_label: whether or not to return one hot version of labels.
      get_label_str: whether or not to return label as text.
      label_offset: If non-zero, this value is subtracted from the parsed label.
        Useful when dataset is 1-indexed.
    """
    modalities.add_image(
        parser_builder=self.parser_builder,
        sampler_builder=self.sampler_builder,
        decoder_builder=self.decoder_builder,
        preprocessor_builder=self.preprocessor_builder,
        postprocessor_builder=self.postprocessor_builder,
        is_training=is_training,
        num_frames=num_frames, stride=stride,
        num_test_clips=num_test_clips,
        min_resize=min_resize, crop_size=crop_size,
        zero_centering_image=zero_centering_image)

    modalities.add_label(
        parser_builder=self.parser_builder,
        decoder_builder=self.decoder_builder,
        preprocessor_builder=self.preprocessor_builder,
        one_hot_label=one_hot_label,
        num_classes=self.num_classes,
        add_label_name=get_label_str)

    if label_offset:
      self.preprocessor_builder.add_fn(
          fn=lambda x: x - label_offset,
          feature_name=builders.LABEL_INDEX_FEATURE_NAME,
          fn_name=f'label_offset_{label_offset}',
          add_before_fn_name=(
              f'{builders.LABEL_INDEX_FEATURE_NAME}_one_hot'))


def load_split(
    ds_factory,
    batch_size: int,
    subset: Text = 'train',
    num_frames: int = 32,
    stride: int = 2,
    num_test_clips: int = 1,
    min_resize: int = 256,
    crop_size: int = 224,
    one_hot_label: bool = True,
    zero_centering: bool = True,
    get_label_str: bool = False,
    augmentation_params: Optional[ml_collections.ConfigDict] = None,
    keep_key: bool = False,
    do_three_spatial_crops: bool = False,
    label_offset: int = 0) -> Tuple[tf.data.Dataset, int]:
  """Loads dataset using DMVR for pre-processing.

  DMVR dataset loader already does basic augmentation (random crop and flip in
    train mode. It also already shuffles and batches the data.

  Args:
    ds_factory: A DMVR factory to instantiate with the subset.
    batch_size: The batch_size to use.
    subset: train, validation or test
    num_frames: Number of frames per subclip.
    stride: Temporal stride to sample frames.
    num_test_clips: Number of test clips (1 by default). If more than 1, this
      will sample multiple linearly spaced clips within each video at test time.
      If 1, then a single clip in the middle of the video is sampled. The clips
      are aggreagated in the batch dimension.
    min_resize: Frames are resized so that min(height, width) is min_resize.
    crop_size: Final size of the frame after cropping the resized frames. Both
      height and width are the same.
    one_hot_label: If True, return one-hot version of the labels (ie [N, C])
      array. Otherwise, return [N]-array of labels.
    zero_centering: If True, frames are normalized to values in the interval
      [-1, 1]. If False, values are in the interval [0, 1].
    get_label_str: whether or not to return label as text.
      Note that strings cannot be used in pmapped functions in Jax!
    augmentation_params: Augmentation configurations in train mode.
    keep_key: bool; If true, also return the key for each example.
    do_three_spatial_crops: If true, take three spatial crops of each clip
      during testing.
    label_offset: If non-zero, this value is subtracted from the parsed label.
      Useful when dataset is 1-indexed.

  Returns:
    A pair `(ds, num_examples)` with
      ds: A `tf.data.Dataset` object
      num_examples: Number of examples in the dataset.
  """
  dataset = ds_factory(subset=subset).configure(
      is_training=(subset == 'train'),
      num_frames=num_frames,
      stride=stride,
      num_test_clips=num_test_clips,
      min_resize=min_resize,
      crop_size=crop_size,
      zero_centering_image=zero_centering,
      one_hot_label=one_hot_label,
      get_label_str=get_label_str,
      label_offset=label_offset)

  if subset == 'train' and augmentation_params:
    dataset = video_ops.additional_augmentations(dataset, augmentation_params,
                                                 crop_size, num_frames,
                                                 zero_centering)

  if subset != 'train' and do_three_spatial_crops:
    rgb_feature_name = builders.IMAGE_FEATURE_NAME

    dataset.preprocessor_builder.replace_fn(
        f'{rgb_feature_name}_central_crop',
        functools.partial(video_ops.three_spatial_crops, crop_size=crop_size))

    if num_test_clips == 1:
      # This means that reshaping is not part of the post-processing graph
      output_shape = (-1, num_frames, crop_size, crop_size, 3)
      dataset.postprocessor_builder.add_fn(
          fn=lambda x: tf.reshape(x, output_shape),
          feature_name=rgb_feature_name,
          fn_name=f'{rgb_feature_name}_reshape')

  logging.info('Preprocessing graph: %s',
               dataset.preprocessor_builder.get_summary())
  logging.info('Postprocessing graph: %s',
               dataset.postprocessor_builder.get_summary())
  num_examples = dataset.num_examples

  ds = dataset.make_dataset(batch_size=batch_size,
                            shuffle=(subset == 'train'),
                            drop_remainder=(subset == 'train'),
                            keep_key=(subset != 'train' and keep_key))

  options = tf.data.Options()
  options.threading.private_threadpool_size = 48
  ds = ds.with_options(options)

  return ds, num_examples


def map_keys(batch: Batch) -> Batch:
  """DMVR dataset returns 'image' and 'label'. We want 'inputs' and 'label'."""

  batch['inputs'] = batch['image']
  return batch


def tile_label_key(batch: Batch) -> Batch:
  """Tile labels and keys to match input videos when num_test_clips > 1.

  When multiple test crops are used (ie num_test_clips > 1), the batch dimension
  of batch['inputs'] = test_batch_size * num_test_clips.
  However, labels and keys remain of size [test_batch_size].
  This function repeats label and key to match the inputs.

  Args:
    batch: Batch from iterator

  Returns:
    Batch with 'label' and 'key' tiled to match 'inputs'. The input batch is
    mutated by the function.
  """
  n_repeats = batch['inputs'].shape[0] // batch['label'].shape[0]
  batch['label'] = np.repeat(batch['label'], n_repeats, axis=0)
  if 'key' in batch:
    batch['key'] = np.repeat(batch['key'], n_repeats, axis=0)
  return batch


@datasets.add_dataset('video_tfrecord_dataset')
def get_dataset(
    *,
    batch_size: int,
    eval_batch_size: int,
    num_shards: int,
    dtype_str: Text = 'float32',
    shuffle_seed: Optional[int] = 0,
    rng: Optional[Rng] = None,
    dataset_configs: ml_collections.ConfigDict,
    dataset_service_address: Optional[str] = None) -> dataset_utils.Dataset:
  """Returns a generator for the dataset."""
  del rng  # Parameter was required by caller API, but is unused.

  def validate_config(field):
    if dataset_configs.get(field) is None:
      raise ValueError(f'{field} must be specified for TFRecord dataset.')
  validate_config('base_dir')
  validate_config('tables')
  validate_config('examples_per_subset')
  validate_config('num_classes')

  num_frames = dataset_configs.get('num_frames', 32)
  num_test_clips = dataset_configs.get('num_test_clips', 1)
  stride = dataset_configs.get('stride', 2)
  min_resize = dataset_configs.get('min_resize', 256)
  crop_size = dataset_configs.get('crop_size', 224)
  one_hot_label = dataset_configs.get('one_hot_label', True)
  zero_centre_data = dataset_configs.get('zero_centering', True)
  augmentation_params = dataset_configs.get('augmentation_params', None)
  num_train_val_clips = dataset_configs.get('num_train_val_clips', 1)
  keep_test_key = dataset_configs.get('keep_test_key', False)
  # For the test set, the actual batch size is
  # test_batch_size * num_test_clips.
  test_batch_size = dataset_configs.get('test_batch_size', eval_batch_size)
  do_three_spatial_crops = dataset_configs.get('do_three_spatial_crops', False)
  num_spatial_crops = 3 if do_three_spatial_crops else 1
  test_split = dataset_configs.get('test_split', 'test')
  label_offset = dataset_configs.get('label_offset', 0)

  ds_factory = functools.partial(
      TFRecordDatasetFactory,
      base_dir=dataset_configs.base_dir,
      tables=dataset_configs.tables,
      examples_per_subset=dataset_configs.examples_per_subset,
      num_classes=dataset_configs.num_classes,
      num_groups=jax.process_count(),
      group_index=jax.process_index())

  def create_dataset_iterator(
      subset: Text,
      batch_size_local: int,
      num_clips: int,
      keep_key_local: bool = False) -> Tuple[Iterator[Batch], int]:
    is_training = subset == 'train'
    is_test = subset == 'test'
    logging.info('Loading split %s', subset)

    dataset, num_examples = load_split(
        ds_factory,
        batch_size=batch_size_local,
        subset=subset,
        num_frames=num_frames,
        stride=stride,
        num_test_clips=num_clips,
        min_resize=min_resize,
        crop_size=crop_size,
        one_hot_label=one_hot_label,
        zero_centering=zero_centre_data,
        augmentation_params=augmentation_params,
        keep_key=keep_key_local,
        do_three_spatial_crops=do_three_spatial_crops and is_test,
        label_offset=label_offset)

    if dataset_service_address and is_training:
      if shuffle_seed is not None:
        raise ValueError('Using dataset service with a random seed causes each '
                         'worker to produce exactly the same data. Add '
                         'config.shuffle_seed = None to your config if you '
                         'want to run with dataset service.')
      logging.info('Using the tf.data service at %s', dataset_service_address)
      dataset = dataset_utils.distribute(dataset, dataset_service_address)

    pad_batch_size = batch_size_local
    if is_test:
      pad_batch_size = batch_size_local * num_clips * num_spatial_crops
    maybe_pad_batches = functools.partial(
        dataset_utils.maybe_pad_batch,
        train=is_training,
        batch_size=pad_batch_size)
    shard_batches = functools.partial(dataset_utils.shard, n_devices=num_shards)

    current_ds_iterator = (
        map_keys(dataset_utils.tf_to_numpy(data)) for data in iter(dataset)
    )

    if is_test and num_clips * num_spatial_crops > 1:
      current_ds_iterator = map(tile_label_key, current_ds_iterator)

    current_ds_iterator = map(maybe_pad_batches, current_ds_iterator)
    current_ds_iterator = map(shard_batches, current_ds_iterator)
    if is_training and dataset_configs.get('prefetch_to_device'):
      # Async bind batch to device which speeds up training.
      current_ds_iterator = jax_utils.prefetch_to_device(
          current_ds_iterator, dataset_configs.get('prefetch_to_device'))

    return current_ds_iterator, num_examples

  train_iter, n_train_examples = create_dataset_iterator(
      'train', batch_size, num_train_val_clips)
  eval_iter, n_eval_examples = create_dataset_iterator(
      'validation', eval_batch_size, num_train_val_clips)
  test_iter, n_test_examples = create_dataset_iterator(
      test_split, test_batch_size, num_test_clips, keep_test_key)

  meta_data = {
      'num_classes': dataset_configs.num_classes,
      'input_shape': (-1, num_frames, crop_size, crop_size, 3),
      'num_train_examples': (n_train_examples * num_train_val_clips),
      'num_eval_examples': (n_eval_examples * num_train_val_clips),
      'num_test_examples':
          (n_test_examples * num_test_clips * num_spatial_crops),
      'input_dtype': getattr(jnp, dtype_str),
      'target_is_onehot': True,
  }
  logging.info('Dataset metadata:\n%s', meta_data)

  return dataset_utils.Dataset(train_iter, eval_iter, test_iter, meta_data)



=== File: /home/ndelafuente/scenic/scenic/projects/vivit/data/__init__.py ===





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/data/tests/test_video_tfrecord_dataset.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Unit tests for datasets."""

from absl.testing import absltest
from absl.testing import parameterized
import jax
import jax.numpy as jnp
import ml_collections
from scenic.projects.vivit.data import video_tfrecord_dataset


class VideoTFRecordDatsetTest(parameterized.TestCase):
  """Unit tests for video_tfrecord_dataset.py."""

  @parameterized.named_parameters(
      ('1 test clip', 1, False, 0),
      ('1x3 test clips', 1, True, 0),
      ('4 test clips, prefetch', 4, False, 1),
      ('4x3 test clips, prefetch', 4, True, 1))
  def test_dataset_builder(self, num_test_clips, do_three_spatial_crops,
                           prefetch_to_device):
    """Tests dataset builder."""
    num_shards = jax.local_device_count()
    batch_size = num_shards * 3
    eval_batch_size = num_shards * 2

    dataset_configs = ml_collections.ConfigDict()
    dataset_configs.prefetch_to_device = prefetch_to_device
    dataset_configs.num_frames = 8
    dataset_configs.num_test_clips = num_test_clips
    dataset_configs.do_three_spatial_crops = do_three_spatial_crops

    dataset_configs.base_dir = '/path/to/dataset_root/'
    dataset_configs.tables = {
        'train': 'something-something-v2-train.rgb.tfrecord@128',
        'validation': 'something-something-v2-validation.rgb.tfrecord@128',
        'test': 'something-something-v2-validation.rgb.tfrecord@128'
    }
    dataset_configs.examples_per_subset = {
        'train': 168913,
        'validation': 24777,
        'test': 24777
    }
    dataset_configs.num_classes = 174

    print('Please set the correct dataset base directory and run'
          'this test again.')
    return

    dataset = video_tfrecord_dataset.get_dataset(
        batch_size=batch_size,
        eval_batch_size=eval_batch_size,
        num_shards=num_shards,
        dataset_configs=dataset_configs)

    self.assertIsNotNone(dataset.train_iter)
    self.assertIsNotNone(dataset.valid_iter)
    self.assertIsNotNone(dataset.test_iter)

    train_batch = next(dataset.train_iter)
    eval_batch = next(dataset.valid_iter)
    test_batch = next(dataset.test_iter)

    # Check shapes.
    num_spatial_crops = 3 if do_three_spatial_crops else 1
    expected_shape = jnp.array((num_shards, batch_size // num_shards) +
                               dataset.meta_data['input_shape'][1:])
    expected_shape_eval = jnp.array(
        (num_shards, eval_batch_size // num_shards) +
        dataset.meta_data['input_shape'][1:])
    expected_shape_test = jnp.array(
        (num_shards,
         eval_batch_size * num_test_clips * num_spatial_crops // num_shards) +
        dataset.meta_data['input_shape'][1:])
    self.assertTrue(
        jnp.array_equal(train_batch['inputs'].shape, expected_shape))
    self.assertTrue(
        jnp.array_equal(eval_batch['inputs'].shape, expected_shape_eval))
    self.assertTrue(
        jnp.array_equal(test_batch['inputs'].shape, expected_shape_test))

    # Check number of examples.
    self.assertEqual(dataset.meta_data['num_train_examples'], 168913)
    self.assertEqual(dataset.meta_data['num_eval_examples'], 24777)
    self.assertEqual(dataset.meta_data['num_test_examples'],
                     24777 * num_test_clips * num_spatial_crops)


if __name__ == '__main__':
  absltest.main()



=== File: /home/ndelafuente/scenic/scenic/projects/vivit/data/tests/__init__.py ===





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/tests/test_vivit_metrics.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Tests for metrics specific to ViViT (ie pairwise accuracy)."""

from absl.testing import absltest
import jax.numpy as jnp
import numpy as np
from scenic.projects.vivit import evaluation_lib
from scenic.projects.vivit import model_utils


class EvaluationMetricsTester(absltest.TestCase):
  """Tests evaluation metrics specific to ViViT model."""

  def test_joint_accuracy(self):
    """Test pairwise accuracy calculation."""

    c_1 = 3
    c_2 = 2
    class_splits = jnp.array([c_1, c_1 + c_2])
    logits = jnp.array([
        [1, 2, 3, 4, 5],
        [1, 2, 3, 4, 5],
        [1, 2, 3, 4, 5],
        [1, 2, 3, 4, 5],
    ])
    one_hot_labels = jnp.array([[0, 1, 0, 1, 0], [0, 0, 1, 1, 0],
                                [0, 1, 0, 0, 1], [0, 0, 1, 0, 1]])

    accuracy = model_utils.joint_accuracy(logits, one_hot_labels, class_splits)
    expected_accuracy = jnp.array([0, 0, 0, 1]).astype(jnp.int32)

    np.testing.assert_almost_equal(
        np.array(expected_accuracy), np.array(accuracy))


class ConfusionMatrixTester(absltest.TestCase):
  """Tests confusion matrix metrics."""

  def test_confusion_matrix_metrics(self):
    """Test calculation of metrics given a confusion matrix."""

    confusion_matrices = [
        np.array([[[2, 0, 1], [1, 3, 0], [1, 2, 4]]]),
        np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]),
    ]
    metrics = evaluation_lib.compute_confusion_matrix_metrics(
        confusion_matrices, return_per_class_metrics=True)

    expected_keys = {'recall/mean', 'precision/mean', 'jaccard/mean',
                     'recall/0', 'recall/1', 'recall/2',
                     'precision/0', 'precision/1', 'precision/2',
                     'jaccard/0', 'jaccard/1', 'jaccard/2'}
    self.assertSameElements(expected_keys, metrics.keys())
    self.assertAlmostEqual(metrics['recall/mean'], np.mean([3/9, 8/19, 13/31]))
    self.assertAlmostEqual(metrics['recall/0'], 3 / 9)
    self.assertAlmostEqual(metrics['recall/1'], 8 / 19)
    self.assertAlmostEqual(metrics['recall/2'], 13 / 31)
    self.assertAlmostEqual(metrics['precision/mean'],
                           np.mean([3 / 16, 8 / 20, 13 / 23]))
    self.assertAlmostEqual(metrics['precision/0'], 3 / 16)
    self.assertAlmostEqual(metrics['precision/1'], 8 / 20)
    self.assertAlmostEqual(metrics['precision/2'], 13 / 23)
    self.assertAlmostEqual(metrics['jaccard/mean'],
                           np.mean([3 / 22, 8 / 31, 13 / 41]))
    self.assertAlmostEqual(metrics['jaccard/0'], 3 / 22)
    self.assertAlmostEqual(metrics['jaccard/1'], 8 / 31)
    self.assertAlmostEqual(metrics['jaccard/2'], 13 / 41)

  def test_with_nans(self):
    """Test metric calculation where one of the metrics is NaN."""

    confusion_matrices = [np.array([[[0, 1], [0, 2]]])]
    metrics = evaluation_lib.compute_confusion_matrix_metrics(
        confusion_matrices, return_per_class_metrics=True)

    self.assertAlmostEqual(metrics['recall/mean'], np.mean([0, 1]))
    self.assertAlmostEqual(metrics['recall/0'], 0)
    self.assertAlmostEqual(metrics['recall/1'], 1)
    self.assertAlmostEqual(metrics['precision/mean'],
                           2 / 3)  #  Should not not average over NaN metrics
    self.assertAlmostEqual(metrics['precision/0'], 0)  # Actually it's NaN
    self.assertAlmostEqual(metrics['precision/1'], 2 / 3)
    self.assertAlmostEqual(metrics['jaccard/mean'], np.mean([0, 2 / 3]))
    self.assertAlmostEqual(metrics['jaccard/0'], 0)
    self.assertAlmostEqual(metrics['jaccard/1'], 2 / 3)

if __name__ == '__main__':
  absltest.main()



=== File: /home/ndelafuente/scenic/scenic/projects/vivit/tests/__init__.py ===





=== File: /home/ndelafuente/scenic/scenic/projects/vivit/tests/test_vivit_trainer.py ===


# Copyright 2024 The Scenic Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Tests for the ViViT classification train script."""

import functools
import shutil
import os

from absl.testing import absltest
from absl.testing import parameterized
import flax
from flax import jax_utils
import flax.linen as nn
import jax.numpy as jnp
import jax.random
import ml_collections
import numpy as np
from scenic.model_lib.base_models import classification_model
from scenic.model_lib.base_models import multilabel_classification_model
from scenic.projects.vivit import evaluation_lib
from scenic.projects.vivit import train_utils as vivit_train_utils
from scenic.train_lib_deprecated import optimizers
from scenic.train_lib_deprecated import train_utils
import tensorflow as tf


class ViViTClassificationTrainerTest(parameterized.TestCase):
  """Tests the default trainer on single device setup."""

  def setUp(self):
    super(ViViTClassificationTrainerTest, self).setUp()
    self.test_dir = '/tmp/scenic_test'
    os.mkdir(self.test_dir)

    # Make sure Tensorflow does not allocate gpu memory.
    tf.config.experimental.set_visible_devices([], 'GPU')

  def tearDown(self):
    shutil.rmtree(self.test_dir)
    super(ViViTClassificationTrainerTest, self).tearDown()

  def get_train_state(self, rng, fake_batch_logits):
    """Generates the initial training state."""
    config = ml_collections.ConfigDict({
        'lr_configs': {
            'base_learning_rate': 0.1,
        },
        'optimizer': 'sgd',
    })

    # Define a fake model that always outputs the same "fake_batch_logits".
    class FakeFlaxModel(nn.Module):
      """A fake flax model."""

      @nn.compact
      def __call__(self, x, train=False, debug=False):
        del x
        del train
        del debug
        # FakeFlaxModule always predicts class 2.
        return fake_batch_logits

    dummy_input = jnp.zeros((10, 10), jnp.float32)
    initial_params = FakeFlaxModel().init(rng, dummy_input).get(
        'params', flax.core.frozen_dict.FrozenDict({}))
    init_model_state = flax.core.frozen_dict.FrozenDict({})
    optimizer = optimizers.get_optimizer(config).create(initial_params)
    init_train_state = jax_utils.replicate(
        train_utils.TrainState(
            global_step=0,
            optimizer=optimizer,
            model_state=init_model_state,
            rng=jax.random.PRNGKey(0)))
    return FakeFlaxModel(), init_train_state

  def train_and_evaluation(self, model, train_state, fake_batches, metrics_fn,
                           return_confusion_matrix=False):
    """Given the train_state, trains the model on fake batches."""
    eval_metrics = []
    fake_batches_replicated = jax_utils.replicate(fake_batches)
    if return_confusion_matrix:
      confusion_matrices = []

    eval_step_pmapped = jax.pmap(
        functools.partial(
            vivit_train_utils.eval_step,
            flax_model=model,
            metrics_fn=metrics_fn,
            return_logits_and_labels=False,
            return_confusion_matrix=return_confusion_matrix,
            debug=False),
        axis_name='batch',
        donate_argnums=(1,),
    )
    for fake_batch in fake_batches_replicated:
      metric_data = eval_step_pmapped(train_state=train_state, batch=fake_batch)
      if return_confusion_matrix:
        metrics, confusion_matrix = metric_data
        confusion_matrices.append(vivit_train_utils.to_cpu(confusion_matrix))
      else:
        metrics = metric_data
      metrics = train_utils.unreplicate_and_get(metrics)
      eval_metrics.append(metrics)
    eval_metrics = train_utils.stack_forest(eval_metrics)
    eval_summary = jax.tree_util.tree_map(lambda x: x.sum(), eval_metrics)
    for key, val in eval_summary.items():
      eval_summary[key] = val[0] / val[1]

    if return_confusion_matrix:
      confusion_matrix_summary = (
          evaluation_lib.compute_confusion_matrix_metrics(
              confusion_matrices, return_per_class_metrics=True))
      return eval_summary, confusion_matrix_summary
    else:
      return eval_summary

  @parameterized.named_parameters(
      ('without confusion matrix summary', False),
      ('with confusion matrix summary', True),
  )
  def test_classifaction_model_evaluate(self, get_confusion_matrix):
    """Test trainer evaluate end to end with classification model metrics."""
    # Define a fixed output for the fake flax model.
    fake_batch_logits = np.tile([.5, .2, .7, 0.0], (4, 1))
    # 4 evaluation batches of size 4.
    fake_batches = [
        {
            'inputs': None,
            'label': np.array([3, 2, 1, 0]),
            'batch_mask': np.array([1, 1, 1, 1])
        },
        {
            'inputs': None,
            'label': np.array([0, 3, 2, 0]),
            'batch_mask': np.array([1, 1, 1, 1])
        },
        {
            'inputs': None,
            'label': np.array([0, 0, 0, 0]),
            'batch_mask': np.array([1, 1, 1, 1])
        },
        {
            'inputs': None,
            'label': np.array([1, 1, 1, 1]),
            'batch_mask': np.array([1, 1, 1, 1])
        },
    ]

    rng = jax.random.PRNGKey(0)
    model, train_state = self.get_train_state(rng, fake_batch_logits)
    eval_summary = self.train_and_evaluation(
        model, train_state, fake_batches,
        functools.partial(
            classification_model.classification_metrics_function,
            target_is_onehot=False),
        get_confusion_matrix)
    if get_confusion_matrix:
      eval_summary, confusion_matrix_summary = eval_summary

    def batch_loss(logits, targets):
      # Softmax cross-entropy loss.
      one_hot_targets = np.eye(4)[targets]
      loss = -np.sum(one_hot_targets * nn.log_softmax(logits), axis=-1)
      return loss

    expected_accuracy = 2.0 / 16.0  # FakeFlaxModule always predicts class 2.
    expected_loss = np.mean(
        [batch_loss(fake_batch_logits, b['label']) for b in fake_batches])

    self.assertEqual(expected_accuracy, eval_summary['accuracy'])
    np.testing.assert_allclose(expected_loss, eval_summary['loss'], atol=1e-6)

    if get_confusion_matrix:
      # As FakeFlaxModule always predicts class 2, this class has a recall of 1,
      # and the denominator for the precision is the total number of examples.
      self.assertAlmostEqual(confusion_matrix_summary['precision/0'], 0.0)
      self.assertAlmostEqual(confusion_matrix_summary['precision/1'], 0.0)
      self.assertAlmostEqual(confusion_matrix_summary['precision/2'], 2. / 16.)
      self.assertAlmostEqual(confusion_matrix_summary['precision/3'], 0.0)
      self.assertAlmostEqual(confusion_matrix_summary['recall/0'], 0.0)
      self.assertAlmostEqual(confusion_matrix_summary['recall/1'], 0.0)
      self.assertAlmostEqual(confusion_matrix_summary['recall/2'], 1.0)
      self.assertAlmostEqual(confusion_matrix_summary['recall/3'], 0.0)
      self.assertAlmostEqual(confusion_matrix_summary['jaccard/0'], 0.0)
      self.assertAlmostEqual(confusion_matrix_summary['jaccard/1'], 0.0)
      self.assertAlmostEqual(confusion_matrix_summary['jaccard/2'], 2. / 16.)
      self.assertAlmostEqual(confusion_matrix_summary['jaccard/3'], 0.0)

  def test_multi_label_classifaction_model_evaluate(self):
    """Test trainer evaluate with multi-label classification model metrics."""
    # Define a fixed output for the fake flax model.
    fake_batch_logits = np.tile([.5, .2, .7, 0.0], (4, 1))
    # 4 evaluation batches of size 4, with multihot labels.
    fake_batches = [
        {
            'inputs':
                None,
            'label':
                np.array([[1, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0],
                          [1, 0, 0, 1]])
        },
        {
            'inputs':
                None,
            'label':
                np.array([[1, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 1],
                          [1, 0, 0, 1]])
        },
        {
            'inputs':
                None,
            'label':
                np.array([[1, 0, 0, 1], [1, 0, 0, 0], [1, 1, 0, 0],
                          [1, 0, 0, 0]])
        },
        {
            'inputs':
                None,
            'label':
                np.array([[0, 1, 0, 1], [0, 1, 0, 0], [1, 1, 0, 0],
                          [0, 1, 0, 0]])
        },
    ]

    rng = jax.random.PRNGKey(0)
    model, train_state = self.get_train_state(rng, fake_batch_logits)
    eval_summary = self.train_and_evaluation(
        model, train_state, fake_batches,
        functools.partial(
            multilabel_classification_model
            .multilabel_classification_metrics_function,
            target_is_multihot=True))

    def batch_loss(logits, multi_hot_targets):
      # Sigmoid cross-entropy loss.
      log_p = jax.nn.log_sigmoid(logits)
      log_not_p = jax.nn.log_sigmoid(-logits)
      loss = -np.sum(
          multi_hot_targets * log_p + (1. - multi_hot_targets) * log_not_p,
          axis=-1)
      return loss

    expected_prec_at_one = 2.0 / 16.0  # FakeFlaxModule always predicts class 2.
    expected_loss = np.mean(
        [batch_loss(fake_batch_logits, b['label']) for b in fake_batches])

    self.assertEqual(expected_prec_at_one, eval_summary['prec@1'])
    np.testing.assert_allclose(expected_loss, eval_summary['loss'], atol=1e-6)


if __name__ == '__main__':
  absltest.main()
